{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb9ffc5",
   "metadata": {},
   "source": [
    "# Project work, part 4\n",
    "Sara H√∏rte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4686130d",
   "metadata": {},
   "source": [
    "### Links\n",
    "\n",
    "GitHub repo link: \n",
    "https://github.com/sarahorte/ind320project.git\n",
    "\n",
    "Streamlit app link: \n",
    "https://ind320project.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7c1b09",
   "metadata": {},
   "source": [
    "### Bonus exercise: \n",
    "Snow drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fceb9b3",
   "metadata": {},
   "source": [
    "### Log\n",
    "\n",
    "M√• jobbe videre med map-siden\n",
    "\n",
    "er her\n",
    "\n",
    "Let the user choose an energy production/energy consumption group and a time interval (in days).\n",
    "\n",
    "    Colour the price areas transparently (e.g., choropleth) according to the mean values over the time interval for the chosen production/consumption.\n",
    "m√• sjekke gjennom den koden.\n",
    "\n",
    "Hente data 2021-2024. slette det som er i mongo fra f√∏r for √• ha nok plass. gj√∏r det f√∏rst. se installation hvs jeg sliter.\n",
    "ogs√• energy consumption.\n",
    "Bytt Java 17 hvis problemer!\n",
    "\n",
    "oppdatere plottene til plotly. tror spectrogram er eneste som mangler det.\n",
    "\n",
    "map osv\n",
    "- use choroplethmap. se pladlet\n",
    "- session 1 17:00 10.nov\n",
    "\n",
    "\n",
    "snow\n",
    "-mulig wind drift ikke er dynamisk, og derfor ikke plotly\n",
    "tror snow er den enkleste oppgaven. \n",
    "\n",
    "\n",
    "\n",
    "organisere sidene.\n",
    "\n",
    "snarimax i streaming_models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b46f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kan v√¶re en nyttig celle for at plotly plots skal bli synelige i Jupyter Notebook, Jupyter Lab og VS Code\n",
    "# The following renders plotly graphs in Jupyter Notebook, Jupyter Lab and VS Code formats\n",
    "#import plotly.io as pio\n",
    "#pio.renderers.default = \"notebook+plotly_mimetype\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8900235",
   "metadata": {},
   "source": [
    "the first thing i did was to fetch the data from 2022-2024 for produciton and 2021-2024 for consumption. i struggled a bit with the connections and getting the databases correct, but not too much. i unfortunaltely managed to delete the already existiting data from 2021 in mongoDB, so i had to go back to CA2 to load this again. then readjusting the code for the new data to not clear the table before loading new data.\n",
    "\n",
    "Updating spectrogram plot and weather plot to use plotly istead of matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed753b8e",
   "metadata": {},
   "source": [
    "### AI usage\n",
    "AI was used extensively throughout this project, primarily through ChatGPT and GitHub Copilot in VS Code. I leveraged AI to generate code suggestions, adapt examples from lectures, and refine solutions for specific tasks. For instance, when implementing the Local Outlier Factor method, I combined code snippets from lectures with AI-generated suggestions, then iteratively adjusted and tested the code until it functioned correctly.\n",
    "\n",
    "I also used AI to interpret error messages and propose potential fixes. By testing different solutions suggested by ChatGPT, I was able to identify the most effective approach. Additionally, AI assisted in generating initial visualizations, which I later fine-tuned to achieve the desired appearance. For parameter selection in the various analysis functions, ChatGPT provided guidance, which was helpful as a starting point, though understanding the underlying concepts is essential for making meaningful choices.\n",
    "\n",
    "The interactive nature of the Streamlit app made it easy to experiment with different parameters, and AI guidance accelerated this process. Beyond coding, I also used ChatGPT to refine and improve the wording of both the project log and the AI usage section itself, ensuring clarity and readability.\n",
    "\n",
    "Overall, AI proved to be a valuable tool for both code development and documentation, complementing my own understanding and enabling more efficient problem-solving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ec980",
   "metadata": {},
   "source": [
    "### Elhub data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eded1006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production data shape: (657600, 4)\n",
      "                          pricearea groupname                   endtime  \\\n",
      "starttime                                                                 \n",
      "2021-12-31 23:00:00+00:00       NO1     hydro 2022-01-01 00:00:00+00:00   \n",
      "2021-12-31 23:00:00+00:00       NO4      wind 2022-01-01 00:00:00+00:00   \n",
      "2021-12-31 23:00:00+00:00       NO2     other 2022-01-01 00:00:00+00:00   \n",
      "2021-12-31 23:00:00+00:00       NO4   thermal 2022-01-01 00:00:00+00:00   \n",
      "2021-12-31 23:00:00+00:00       NO1   thermal 2022-01-01 00:00:00+00:00   \n",
      "\n",
      "                           quantitykwh  \n",
      "starttime                               \n",
      "2021-12-31 23:00:00+00:00   1291422.40  \n",
      "2021-12-31 23:00:00+00:00    320912.40  \n",
      "2021-12-31 23:00:00+00:00         0.20  \n",
      "2021-12-31 23:00:00+00:00     38372.56  \n",
      "2021-12-31 23:00:00+00:00     30049.92  \n",
      "Consumption data shape: (876600, 4)\n",
      "                          pricearea  groupname                   endtime  \\\n",
      "starttime                                                                  \n",
      "2020-12-31 23:00:00+00:00       NO1      cabin 2021-01-01 00:00:00+00:00   \n",
      "2020-12-31 23:00:00+00:00       NO4   tertiary 2021-01-01 00:00:00+00:00   \n",
      "2020-12-31 23:00:00+00:00       NO2  household 2021-01-01 00:00:00+00:00   \n",
      "2020-12-31 23:00:00+00:00       NO4  secondary 2021-01-01 00:00:00+00:00   \n",
      "2020-12-31 23:00:00+00:00       NO1  secondary 2021-01-01 00:00:00+00:00   \n",
      "\n",
      "                           quantitykwh  \n",
      "starttime                               \n",
      "2020-12-31 23:00:00+00:00    177071.56  \n",
      "2020-12-31 23:00:00+00:00    366046.00  \n",
      "2020-12-31 23:00:00+00:00   1425141.00  \n",
      "2020-12-31 23:00:00+00:00    848937.56  \n",
      "2020-12-31 23:00:00+00:00    693011.75  \n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# FETCH DATA FROM ELHUB API (2021-2024) ‚Äì FASTER\n",
    "# =============================\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- API SETTINGS ---\n",
    "BASE_URL = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "\n",
    "# datasets\n",
    "DATASET_PROD = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "DATASET_CONS = \"CONSUMPTION_PER_GROUP_MBA_HOUR\"\n",
    "\n",
    "# --- DATE RANGES ---\n",
    "PROD_START = datetime(2022, 1, 1)\n",
    "PROD_END   = datetime(2024, 12, 31)\n",
    "\n",
    "CONS_START = datetime(2021, 1, 1)\n",
    "CONS_END   = datetime(2024, 12, 31)\n",
    "\n",
    "# --- FUNCTION TO FORMAT DATES ---\n",
    "def format_date(dt_obj):\n",
    "    \"\"\"Formats datetime for Elhub API (+02:00 offset).\"\"\"\n",
    "    return dt_obj.strftime(\"%Y-%m-%dT%H:%M:%S%%2B02:00\")  # +02:00 encoded\n",
    "\n",
    "HEADERS = {\"Accept\": \"application/json\"}\n",
    "\n",
    "# -----------------------\n",
    "# Fetch data for a given month\n",
    "# -----------------------\n",
    "def fetch_month(dataset, year, month):\n",
    "    start = datetime(year, month, 1)\n",
    "    next_month = (start + timedelta(days=32)).replace(day=1)\n",
    "    end = next_month - timedelta(seconds=1)\n",
    "\n",
    "    url = f\"{BASE_URL}?dataset={dataset}&startDate={format_date(start)}&endDate={format_date(end)}\"\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=60)\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"‚ùå Error {resp.status_code} for {start.date()} ‚Üí {end.date()}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    data = resp.json().get(\"data\", [])\n",
    "    records = []\n",
    "    for entry in data:\n",
    "        attrs = entry.get(\"attributes\", {})\n",
    "        if dataset == DATASET_PROD:\n",
    "            key = \"productionPerGroupMbaHour\"\n",
    "            group_field = \"productionGroup\"\n",
    "        else:\n",
    "            key = \"consumptionPerGroupMbaHour\"\n",
    "            group_field = \"consumptionGroup\"\n",
    "        recs = attrs.get(key, [])\n",
    "        recs = [r for r in recs if r.get(group_field) != \"*\" and r.get(group_field) is not None]\n",
    "        for r in recs:\n",
    "            records.append({\n",
    "                \"pricearea\": r.get(\"priceArea\"),\n",
    "                \"groupname\": r.get(group_field),\n",
    "                \"starttime\": r.get(\"startTime\"),\n",
    "                \"endtime\": r.get(\"endTime\"),\n",
    "                \"quantitykwh\": r.get(\"quantityKwh\")\n",
    "            })\n",
    "    if records:\n",
    "        df = pd.DataFrame(records)\n",
    "        df['starttime'] = pd.to_datetime(df['starttime'], utc=True, errors='coerce')\n",
    "        df['endtime'] = pd.to_datetime(df['endtime'], utc=True, errors='coerce')\n",
    "        df['quantitykwh'] = pd.to_numeric(df['quantitykwh'], errors='coerce')\n",
    "        df = df[['pricearea', 'groupname', 'starttime', 'endtime', 'quantitykwh']]\n",
    "        df.sort_values('starttime', inplace=True)\n",
    "        df.set_index('starttime', inplace=True)\n",
    "        return df\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# -----------------------\n",
    "# Fetch all months in a range\n",
    "# -----------------------\n",
    "def fetch_range_monthly(dataset, start_dt, end_dt):\n",
    "    all_dfs = []\n",
    "    cur = start_dt\n",
    "    while cur <= end_dt:\n",
    "        df_month = fetch_month(dataset, cur.year, cur.month)\n",
    "        if not df_month.empty:\n",
    "            all_dfs.append(df_month)\n",
    "        # move to next month\n",
    "        next_month = (cur + timedelta(days=32)).replace(day=1)\n",
    "        cur = next_month\n",
    "    return pd.concat(all_dfs) if all_dfs else pd.DataFrame()\n",
    "\n",
    "# -----------------------\n",
    "# Run ingestion\n",
    "# -----------------------\n",
    "# Production 2022‚Äì2024\n",
    "df_prod = fetch_range_monthly(DATASET_PROD, PROD_START, PROD_END)\n",
    "print(\"Production data shape:\", df_prod.shape)\n",
    "print(df_prod.head())\n",
    "\n",
    "# Consumption 2021‚Äì2024\n",
    "df_cons = fetch_range_monthly(DATASET_CONS, CONS_START, CONS_END)\n",
    "print(\"Consumption data shape:\", df_cons.shape)\n",
    "print(df_cons.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f282abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production DataFrame ready: (657600, 4)\n",
      "Consumption DataFrame ready: (876600, 4)\n",
      "Production group names: ['hydro' 'wind' 'other' 'thermal' 'solar']\n",
      "Consumption group names: ['cabin' 'tertiary' 'household' 'secondary' 'primary']\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# FIX DATAFRAMES FOR SPARK / CASSANDRA\n",
    "# ============================\n",
    "\n",
    "# 1Ô∏è‚É£ Reset index so 'starttime' becomes a column\n",
    "df_prod = df_prod.reset_index()   # 'starttime' moves from index ‚Üí column\n",
    "df_cons = df_cons.reset_index()\n",
    "\n",
    "# 2Ô∏è‚É£ Strip any extra whitespace from column names\n",
    "df_prod.columns = df_prod.columns.str.strip()\n",
    "df_cons.columns = df_cons.columns.str.strip()\n",
    "\n",
    "# 3Ô∏è‚É£ Ensure uniform group column name\n",
    "# (Already 'groupname', but this covers any old variations)\n",
    "if 'productiongroup' in df_prod.columns:\n",
    "    df_prod = df_prod.rename(columns={\"productiongroup\": \"groupname\"})\n",
    "if 'consumptiongroup' in df_cons.columns:\n",
    "    df_cons = df_cons.rename(columns={\"consumptiongroup\": \"groupname\"})\n",
    "\n",
    "# 4Ô∏è‚É£ Ensure proper types\n",
    "df_prod['starttime'] = pd.to_datetime(df_prod['starttime'], utc=True, errors='coerce')\n",
    "df_prod['quantitykwh'] = pd.to_numeric(df_prod['quantitykwh'], errors='coerce')\n",
    "\n",
    "df_cons['starttime'] = pd.to_datetime(df_cons['starttime'], utc=True, errors='coerce')\n",
    "df_cons['quantitykwh'] = pd.to_numeric(df_cons['quantitykwh'], errors='coerce')\n",
    "\n",
    "# 5Ô∏è‚É£ Drop rows with missing critical values\n",
    "df_prod = df_prod.dropna(subset=['starttime','pricearea','groupname','quantitykwh'])\n",
    "df_cons = df_cons.dropna(subset=['starttime','pricearea','groupname','quantitykwh'])\n",
    "\n",
    "# 6Ô∏è‚É£ Optional: drop 'endtime' if not needed in Cassandra\n",
    "df_prod = df_prod.drop(columns=['endtime'], errors='ignore')\n",
    "df_cons = df_cons.drop(columns=['endtime'], errors='ignore')\n",
    "\n",
    "# ‚úÖ Check that DataFrames are ready\n",
    "print(\"Production DataFrame ready:\", df_prod.shape)\n",
    "print(\"Consumption DataFrame ready:\", df_cons.shape)\n",
    "print(\"Production group names:\", df_prod['groupname'].unique())\n",
    "print(\"Consumption group names:\", df_cons['groupname'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a515369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Keyspace and tables for production and consumption are ready\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "# Connect to local Cassandra\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "session = cluster.connect()\n",
    "\n",
    "# --- Create keyspace ---\n",
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS energy\n",
    "    WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\n",
    "\"\"\")\n",
    "\n",
    "# --- Create table for production ---\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS energy.production_per_group (\n",
    "        pricearea text,\n",
    "        starttime timestamp,\n",
    "        groupname text,\n",
    "        quantitykwh double,\n",
    "        PRIMARY KEY ((pricearea), starttime, groupname)\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# --- Create table for consumption ---\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS energy.consumption_per_group (\n",
    "        pricearea text,\n",
    "        starttime timestamp,\n",
    "        groupname text,\n",
    "        quantitykwh double,\n",
    "        PRIMARY KEY ((pricearea), starttime, groupname)\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Keyspace and tables for production and consumption are ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebbf7976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ starttime timezone fixed to UTC for both DataFrames\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Ensure starttime is timezone-aware UTC\n",
    "# -------------------------------\n",
    "\n",
    "# Production\n",
    "if df_prod['starttime'].dt.tz is None:\n",
    "    df_prod['starttime'] = df_prod['starttime'].dt.tz_localize('UTC')\n",
    "else:\n",
    "    df_prod['starttime'] = df_prod['starttime'].dt.tz_convert('UTC')\n",
    "\n",
    "# Consumption\n",
    "if df_cons['starttime'].dt.tz is None:\n",
    "    df_cons['starttime'] = df_cons['starttime'].dt.tz_localize('UTC')\n",
    "else:\n",
    "    df_cons['starttime'] = df_cons['starttime'].dt.tz_convert('UTC')\n",
    "\n",
    "print(\"‚úÖ starttime timezone fixed to UTC for both DataFrames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ed07029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/envs/D2D_env/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/sarahorte/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/sarahorte/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9e1e1ede-c2d5-45c7-aed7-d58cb73df9f2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      ":: resolution report :: resolve 171ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.5.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   19  |   0   |   0   |   0   ||   19  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9e1e1ede-c2d5-45c7-aed7-d58cb73df9f2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 19 already retrieved (0kB/4ms)\n",
      "25/11/19 12:11:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/19 12:11:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n",
      "Writing 657600 rows in 7 chunks for production...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 12:11:55 WARN TaskSetManager: Stage 0 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 1/7 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 12:11:57 WARN TaskSetManager: Stage 1 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 2/7 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 12:11:59 WARN TaskSetManager: Stage 2 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 3/7 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 12:12:01 WARN TaskSetManager: Stage 3 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 4/7 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 12:12:03 WARN TaskSetManager: Stage 4 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 5/7 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 12:12:04 WARN TaskSetManager: Stage 5 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 6/7 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 12:12:06 WARN TaskSetManager: Stage 6 contains a task of very large size (1993 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 7/7 written (57600 rows)\n",
      "Writing 876600 rows in 9 chunks for consumption...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 12:12:07 WARN TaskSetManager: Stage 7 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 1/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 12:12:09 WARN TaskSetManager: Stage 8 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 2/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 12:12:10 WARN TaskSetManager: Stage 9 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 3/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 12:12:12 WARN TaskSetManager: Stage 10 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 4/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 12:12:13 WARN TaskSetManager: Stage 11 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 5/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 12:12:15 WARN TaskSetManager: Stage 12 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 6/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 12:12:16 WARN TaskSetManager: Stage 13 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 7/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 12:12:18 WARN TaskSetManager: Stage 14 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 8/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 12:12:20 WARN TaskSetManager: Stage 15 contains a task of very large size (2826 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 9/9 written (76600 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType\n",
    "import math\n",
    "\n",
    "# --- CREATE SPARK SESSION ---\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ElhubDataIngest\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "# --- DEFINE COMMON SCHEMA ---\n",
    "schema = StructType([\n",
    "    StructField(\"pricearea\", StringType(), True),\n",
    "    StructField(\"groupname\", StringType(), True),\n",
    "    StructField(\"starttime\", TimestampType(), True),\n",
    "    StructField(\"quantitykwh\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# --- FUNCTION TO WRITE IN CHUNKS ---\n",
    "def write_to_cassandra_fast(df_pandas, dataset_type=\"production\", chunk_size=100_000):\n",
    "    if df_pandas is None or df_pandas.empty:\n",
    "        print(f\"No data to write for {dataset_type}\")\n",
    "        return\n",
    "\n",
    "    # Drop leftover index columns and lowercase all columns\n",
    "    df_pandas = df_pandas.drop(columns=['index', 'level_0'], errors='ignore')\n",
    "    df_pandas.columns = [c.lower() for c in df_pandas.columns]\n",
    "\n",
    "    # Drop rows with missing critical values\n",
    "    df_pandas = df_pandas.dropna(subset=['pricearea', 'groupname', 'starttime', 'quantitykwh'])\n",
    "\n",
    "    total_rows = len(df_pandas)\n",
    "    n_chunks = math.ceil(total_rows / chunk_size)\n",
    "    print(f\"Writing {total_rows} rows in {n_chunks} chunks for {dataset_type}...\")\n",
    "\n",
    "    for i in range(n_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i+1) * chunk_size, total_rows)\n",
    "        df_chunk = df_pandas.iloc[start_idx:end_idx]\n",
    "\n",
    "        # Reorder columns for schema\n",
    "        df_chunk_spark = df_chunk[['pricearea', 'groupname', 'starttime', 'quantitykwh']]\n",
    "\n",
    "        # Convert to Spark DataFrame\n",
    "        spark_df = spark.createDataFrame(df_chunk_spark, schema=schema)\n",
    "\n",
    "        # Rename columns for Cassandra\n",
    "        if dataset_type == \"production\":\n",
    "            spark_df_cassandra = spark_df.selectExpr(\n",
    "                \"pricearea\",\n",
    "                \"starttime\",\n",
    "                \"groupname as productiongroup\",\n",
    "                \"quantitykwh\"\n",
    "            )\n",
    "            table = \"production_per_group\"\n",
    "        else:\n",
    "            spark_df_cassandra = spark_df.selectExpr(\n",
    "                \"pricearea\",\n",
    "                \"starttime\",\n",
    "                \"groupname as consumptiongroup\",\n",
    "                \"quantitykwh\"\n",
    "            )\n",
    "            table = \"consumption_per_group\"\n",
    "\n",
    "        # Reduce partitions for faster writes\n",
    "        spark_df_cassandra.coalesce(1).write \\\n",
    "            .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "            .options(keyspace=\"energy\", table=table) \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "\n",
    "        print(f\"‚úÖ Chunk {i+1}/{n_chunks} written ({len(df_chunk)} rows)\")\n",
    "\n",
    "# --- WRITE DATASETS ---\n",
    "write_to_cassandra_fast(df_prod, \"production\")\n",
    "write_to_cassandra_fast(df_cons, \"consumption\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4321e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Production table preview:\n",
      "+---------+-------------------+---------------+-----------+\n",
      "|pricearea|starttime          |productiongroup|quantitykwh|\n",
      "+---------+-------------------+---------------+-----------+\n",
      "|NO1      |2021-01-01 00:00:00|hydro          |2507716.8  |\n",
      "|NO1      |2021-01-01 00:00:00|other          |0.0        |\n",
      "|NO1      |2021-01-01 00:00:00|solar          |6.106      |\n",
      "|NO1      |2021-01-01 00:00:00|thermal        |51369.035  |\n",
      "|NO1      |2021-01-01 00:00:00|wind           |937.072    |\n",
      "+---------+-------------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows in production_per_group: 872853\n",
      "\n",
      "‚úÖ Consumption table preview:\n",
      "+---------+-------------------+----------------+-----------+\n",
      "|pricearea|starttime          |consumptiongroup|quantitykwh|\n",
      "+---------+-------------------+----------------+-----------+\n",
      "|NO1      |2021-01-01 00:00:00|cabin           |177071.56  |\n",
      "|NO1      |2021-01-01 00:00:00|household       |2366888.8  |\n",
      "|NO1      |2021-01-01 00:00:00|primary         |70166.62   |\n",
      "|NO1      |2021-01-01 00:00:00|secondary       |693011.75  |\n",
      "|NO1      |2021-01-01 00:00:00|tertiary        |1123886.6  |\n",
      "+---------+-------------------+----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows in consumption_per_group: 876500\n",
      "\n",
      "Production groups in Cassandra: [Row(productiongroup='solar'), Row(productiongroup='other'), Row(productiongroup='thermal'), Row(productiongroup='hydro'), Row(productiongroup='wind')]\n",
      "Consumption groups in Cassandra: [Row(consumptiongroup='tertiary'), Row(consumptiongroup='secondary'), Row(consumptiongroup='household'), Row(consumptiongroup='cabin'), Row(consumptiongroup='primary')]\n",
      "Distinct production groups: ['solar', 'other', 'thermal', 'hydro', 'wind']\n",
      "Distinct consumption groups: ['tertiary', 'secondary', 'household', 'cabin', 'primary']\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3Ô∏è‚É£ READ DATA BACK FROM CASSANDRA\n",
    "# ==========================================\n",
    "def read_from_cassandra(table_name):\n",
    "    \"\"\"Reads a table from the 'energy' keyspace into a Spark DataFrame.\"\"\"\n",
    "    df = spark.read \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(keyspace=\"energy\", table=table_name) \\\n",
    "        .load()\n",
    "    return df\n",
    "\n",
    "# --- Read production table ---\n",
    "cass_prod_df = read_from_cassandra(\"production_per_group\")\n",
    "print(\"‚úÖ Production table preview:\")\n",
    "cass_prod_df.show(5, truncate=False)\n",
    "\n",
    "prod_count = cass_prod_df.count()\n",
    "print(f\"Total rows in production_per_group: {prod_count}\")\n",
    "\n",
    "# --- Read consumption table ---\n",
    "cass_cons_df = read_from_cassandra(\"consumption_per_group\")\n",
    "print(\"\\n‚úÖ Consumption table preview:\")\n",
    "cass_cons_df.show(5, truncate=False)\n",
    "\n",
    "cons_count = cass_cons_df.count()\n",
    "print(f\"Total rows in consumption_per_group: {cons_count}\")\n",
    "\n",
    "# --- Optional: basic sanity check on groups ---\n",
    "print(\"\\nProduction groups in Cassandra:\", cass_prod_df.select(\"productiongroup\").distinct().collect())\n",
    "print(\"Consumption groups in Cassandra:\", cass_cons_df.select(\"consumptiongroup\").distinct().collect())\n",
    "\n",
    "\n",
    "print(\"Distinct production groups:\", [row['productiongroup'] for row in cass_prod_df.select('productiongroup').distinct().collect()])\n",
    "print(\"Distinct consumption groups:\", [row['consumptiongroup'] for row in cass_cons_df.select('consumptiongroup').distinct().collect()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18ee26b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient, UpdateOne\n",
    "import math\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# === MongoDB Atlas connection ===\n",
    "uri = \"mongodb+srv://{}:{}@cluster0.qwrlccf.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "USR, PWD = open('/Users/sarahorte/Documents/IND320/Personlig/No_sync/MongoDB').read().splitlines()\n",
    "client = MongoClient(uri.format(USR, PWD))\n",
    "database = client['elhub']\n",
    "\n",
    "def upsert_chunk(df_chunk, collection_name, group_field):\n",
    "    collection = database[collection_name]\n",
    "    records = df_chunk.to_dict(orient='records')\n",
    "    operations = [\n",
    "        UpdateOne(\n",
    "            {\n",
    "                \"pricearea\": rec[\"pricearea\"],\n",
    "                \"starttime\": rec[\"starttime\"],\n",
    "                group_field: rec[group_field]\n",
    "            },\n",
    "            {\"$set\": rec},\n",
    "            upsert=True\n",
    "        )\n",
    "        for rec in records\n",
    "    ]\n",
    "    if operations:\n",
    "        collection.bulk_write(operations)\n",
    "    return len(df_chunk)  # for progress tracking\n",
    "\n",
    "def upsert_into_mongo_parallel(df, collection_name, group_field, chunk_size=50_000, max_workers=4):\n",
    "    total_rows = len(df)\n",
    "    n_chunks = math.ceil(total_rows / chunk_size)\n",
    "    print(f\"Upserting {total_rows} documents to '{collection_name}' in {n_chunks} chunks using {max_workers} workers...\")\n",
    "\n",
    "    chunks = [df.iloc[i*chunk_size : min((i+1)*chunk_size, total_rows)] for i in range(n_chunks)]\n",
    "    inserted_so_far = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(upsert_chunk, chunk, collection_name, group_field): i for i, chunk in enumerate(chunks)}\n",
    "        for future in as_completed(futures):\n",
    "            inserted = future.result()\n",
    "            inserted_so_far += inserted\n",
    "            percent = inserted_so_far / total_rows * 100\n",
    "            print(f\"‚úÖ {inserted_so_far}/{total_rows} documents upserted ({percent:.1f}% done)\")\n",
    "\n",
    "    total_docs = database[collection_name].count_documents({})\n",
    "    print(f\"‚úÖ Finished. Total documents in '{collection_name}': {total_docs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0c76829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 657600 documents to 'production_data' in 7 chunks...\n",
      "‚úÖ Chunk 1/7 inserted (100000 docs)\n",
      "‚úÖ Chunk 2/7 inserted (100000 docs)\n",
      "‚úÖ Chunk 3/7 inserted (100000 docs)\n",
      "‚úÖ Chunk 4/7 inserted (100000 docs)\n",
      "‚úÖ Chunk 5/7 inserted (100000 docs)\n",
      "‚úÖ Chunk 6/7 inserted (100000 docs)\n",
      "‚úÖ Chunk 7/7 inserted (57600 docs)\n",
      "‚úÖ Total documents in 'production_data': 872953\n",
      "üóë Cleared existing data in 'consumption_data'\n",
      "Writing 876600 documents to 'consumption_data' in 9 chunks...\n",
      "‚úÖ Chunk 1/9 inserted (100000 docs)\n",
      "‚úÖ Chunk 2/9 inserted (100000 docs)\n",
      "‚úÖ Chunk 3/9 inserted (100000 docs)\n",
      "‚úÖ Chunk 4/9 inserted (100000 docs)\n",
      "‚úÖ Chunk 5/9 inserted (100000 docs)\n",
      "‚úÖ Chunk 6/9 inserted (100000 docs)\n",
      "‚úÖ Chunk 7/9 inserted (100000 docs)\n",
      "‚úÖ Chunk 8/9 inserted (100000 docs)\n",
      "‚úÖ Chunk 9/9 inserted (76600 docs)\n",
      "‚úÖ Total documents in 'consumption_data': 876600\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import math\n",
    "\n",
    "# === MongoDB Atlas connection ===\n",
    "uri = \"mongodb+srv://{}:{}@cluster0.qwrlccf.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "USR, PWD = open('/Users/sarahorte/Documents/IND320/Personlig/No_sync/MongoDB').read().splitlines()\n",
    "client = MongoClient(uri.format(USR, PWD))\n",
    "database = client['elhub']\n",
    "\n",
    "# --- Fast bulk insert function ---\n",
    "def bulk_insert(df, collection_name, chunk_size=100_000, clear_existing=False):\n",
    "    collection = database[collection_name]\n",
    "    if clear_existing:\n",
    "        collection.delete_many({})\n",
    "        print(f\"üóë Cleared existing data in '{collection_name}'\")\n",
    "\n",
    "    total_rows = len(df)\n",
    "    n_chunks = math.ceil(total_rows / chunk_size)\n",
    "    print(f\"Writing {total_rows} documents to '{collection_name}' in {n_chunks} chunks...\")\n",
    "\n",
    "    for i in range(n_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i+1) * chunk_size, total_rows)\n",
    "        df_chunk = df.iloc[start_idx:end_idx]\n",
    "        records = df_chunk.to_dict(orient='records')\n",
    "        collection.insert_many(records)\n",
    "        print(f\"‚úÖ Chunk {i+1}/{n_chunks} inserted ({len(df_chunk)} docs)\")\n",
    "\n",
    "    total_docs = collection.count_documents({})\n",
    "    print(f\"‚úÖ Total documents in '{collection_name}': {total_docs}\")\n",
    "\n",
    "# --- Prepare DataFrames for MongoDB ---\n",
    "df_prod_mongo = df_prod.copy()  # production data\n",
    "df_cons_mongo = df_cons.copy()  # consumption data\n",
    "\n",
    "# --- Run fast insertion ---\n",
    "bulk_insert(df_prod_mongo, 'production_data', clear_existing=False)  # append new production\n",
    "bulk_insert(df_cons_mongo, 'consumption_data', clear_existing=True)  # reload consumption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a578f2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2D_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
