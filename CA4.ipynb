{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb9ffc5",
   "metadata": {},
   "source": [
    "# Project work, part 4\n",
    "Sara Hørte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4686130d",
   "metadata": {},
   "source": [
    "### Links\n",
    "\n",
    "GitHub repo link: \n",
    "https://github.com/sarahorte/ind320project.git\n",
    "\n",
    "Streamlit app link: \n",
    "https://ind320project.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7c1b09",
   "metadata": {},
   "source": [
    "### Bonus exercise: \n",
    "Snow drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fceb9b3",
   "metadata": {},
   "source": [
    "### Log\n",
    "\n",
    "Hente data 2021-2024. slette det som er i mongo fra før for å ha nok plass. gjør det først. se installation hvs jeg sliter.\n",
    "også energy consumption.\n",
    "Bytt Java 17 hvis problemer!\n",
    "\n",
    "oppdatere plottene til plotly. tror spectrogram er eneste som mangler det.\n",
    "\n",
    "map osv\n",
    "- use choroplethmap. se pladlet\n",
    "- session 1 17:00 10.nov\n",
    "\n",
    "\n",
    "snow\n",
    "-mulig wind drift ikke er dynamisk, og derfor ikke plotly\n",
    "tror snow er den enkleste oppgaven. \n",
    "\n",
    "\n",
    "\n",
    "organisere sidene.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed753b8e",
   "metadata": {},
   "source": [
    "### AI usage\n",
    "AI was used extensively throughout this project, primarily through ChatGPT and GitHub Copilot in VS Code. I leveraged AI to generate code suggestions, adapt examples from lectures, and refine solutions for specific tasks. For instance, when implementing the Local Outlier Factor method, I combined code snippets from lectures with AI-generated suggestions, then iteratively adjusted and tested the code until it functioned correctly.\n",
    "\n",
    "I also used AI to interpret error messages and propose potential fixes. By testing different solutions suggested by ChatGPT, I was able to identify the most effective approach. Additionally, AI assisted in generating initial visualizations, which I later fine-tuned to achieve the desired appearance. For parameter selection in the various analysis functions, ChatGPT provided guidance, which was helpful as a starting point, though understanding the underlying concepts is essential for making meaningful choices.\n",
    "\n",
    "The interactive nature of the Streamlit app made it easy to experiment with different parameters, and AI guidance accelerated this process. Beyond coding, I also used ChatGPT to refine and improve the wording of both the project log and the AI usage section itself, ensuring clarity and readability.\n",
    "\n",
    "Overall, AI proved to be a valuable tool for both code development and documentation, complementing my own understanding and enabling more efficient problem-solving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ec980",
   "metadata": {},
   "source": [
    "### Elhub data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eded1006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production data shape: (657600, 4)\n",
      "                          priceArea groupName                   endTime  \\\n",
      "startTime                                                                 \n",
      "2021-12-31 23:00:00+00:00       NO1     hydro 2022-01-01 00:00:00+00:00   \n",
      "2021-12-31 23:00:00+00:00       NO4      wind 2022-01-01 00:00:00+00:00   \n",
      "2021-12-31 23:00:00+00:00       NO2     other 2022-01-01 00:00:00+00:00   \n",
      "2021-12-31 23:00:00+00:00       NO4   thermal 2022-01-01 00:00:00+00:00   \n",
      "2021-12-31 23:00:00+00:00       NO1   thermal 2022-01-01 00:00:00+00:00   \n",
      "\n",
      "                           quantityKwh  \n",
      "startTime                               \n",
      "2021-12-31 23:00:00+00:00   1291422.40  \n",
      "2021-12-31 23:00:00+00:00    320912.40  \n",
      "2021-12-31 23:00:00+00:00         0.20  \n",
      "2021-12-31 23:00:00+00:00     38372.56  \n",
      "2021-12-31 23:00:00+00:00     30049.92  \n",
      "Consumption data shape: (876600, 4)\n",
      "                          priceArea  groupName                   endTime  \\\n",
      "startTime                                                                  \n",
      "2020-12-31 23:00:00+00:00       NO1      cabin 2021-01-01 00:00:00+00:00   \n",
      "2020-12-31 23:00:00+00:00       NO4   tertiary 2021-01-01 00:00:00+00:00   \n",
      "2020-12-31 23:00:00+00:00       NO2  household 2021-01-01 00:00:00+00:00   \n",
      "2020-12-31 23:00:00+00:00       NO4  secondary 2021-01-01 00:00:00+00:00   \n",
      "2020-12-31 23:00:00+00:00       NO1  secondary 2021-01-01 00:00:00+00:00   \n",
      "\n",
      "                           quantityKwh  \n",
      "startTime                               \n",
      "2020-12-31 23:00:00+00:00    177071.56  \n",
      "2020-12-31 23:00:00+00:00    366046.00  \n",
      "2020-12-31 23:00:00+00:00   1425141.00  \n",
      "2020-12-31 23:00:00+00:00    848937.56  \n",
      "2020-12-31 23:00:00+00:00    693011.75  \n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# FETCH DATA FROM ELHUB API (2021-2024) – FASTER\n",
    "# =============================\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- API SETTINGS ---\n",
    "BASE_URL = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "\n",
    "# datasets\n",
    "DATASET_PROD = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "DATASET_CONS = \"CONSUMPTION_PER_GROUP_MBA_HOUR\"\n",
    "\n",
    "# --- DATE RANGES ---\n",
    "PROD_START = datetime(2022, 1, 1)\n",
    "PROD_END   = datetime(2024, 12, 31)\n",
    "\n",
    "CONS_START = datetime(2021, 1, 1)\n",
    "CONS_END   = datetime(2024, 12, 31)\n",
    "\n",
    "# --- FUNCTION TO FORMAT DATES ---\n",
    "def format_date(dt_obj):\n",
    "    \"\"\"Formats datetime for Elhub API (+02:00 offset).\"\"\"\n",
    "    return dt_obj.strftime(\"%Y-%m-%dT%H:%M:%S%%2B02:00\")  # +02:00 encoded\n",
    "\n",
    "HEADERS = {\"Accept\": \"application/json\"}\n",
    "\n",
    "# -----------------------\n",
    "# Fetch data for a given month\n",
    "# -----------------------\n",
    "def fetch_month(dataset, year, month):\n",
    "    start = datetime(year, month, 1)\n",
    "    next_month = (start + timedelta(days=32)).replace(day=1)\n",
    "    end = next_month - timedelta(seconds=1)\n",
    "\n",
    "    url = f\"{BASE_URL}?dataset={dataset}&startDate={format_date(start)}&endDate={format_date(end)}\"\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=60)\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"❌ Error {resp.status_code} for {start.date()} → {end.date()}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    data = resp.json().get(\"data\", [])\n",
    "    records = []\n",
    "    for entry in data:\n",
    "        attrs = entry.get(\"attributes\", {})\n",
    "        if dataset == DATASET_PROD:\n",
    "            key = \"productionPerGroupMbaHour\"\n",
    "            group_field = \"productionGroup\"\n",
    "        else:\n",
    "            key = \"consumptionPerGroupMbaHour\"\n",
    "            group_field = \"consumptionGroup\"\n",
    "        recs = attrs.get(key, [])\n",
    "        recs = [r for r in recs if r.get(group_field) != \"*\" and r.get(group_field) is not None]\n",
    "        for r in recs:\n",
    "            records.append({\n",
    "                \"priceArea\": r.get(\"priceArea\"),\n",
    "                \"groupName\": r.get(group_field),\n",
    "                \"startTime\": r.get(\"startTime\"),\n",
    "                \"endTime\": r.get(\"endTime\"),\n",
    "                \"quantityKwh\": r.get(\"quantityKwh\")\n",
    "            })\n",
    "    if records:\n",
    "        df = pd.DataFrame(records)\n",
    "        df['startTime'] = pd.to_datetime(df['startTime'], utc=True, errors='coerce')\n",
    "        df['endTime'] = pd.to_datetime(df['endTime'], utc=True, errors='coerce')\n",
    "        df['quantityKwh'] = pd.to_numeric(df['quantityKwh'], errors='coerce')\n",
    "        df = df[['priceArea', 'groupName', 'startTime', 'endTime', 'quantityKwh']]\n",
    "        df.sort_values('startTime', inplace=True)\n",
    "        df.set_index('startTime', inplace=True)\n",
    "        return df\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# -----------------------\n",
    "# Fetch all months in a range\n",
    "# -----------------------\n",
    "def fetch_range_monthly(dataset, start_dt, end_dt):\n",
    "    all_dfs = []\n",
    "    cur = start_dt\n",
    "    while cur <= end_dt:\n",
    "        df_month = fetch_month(dataset, cur.year, cur.month)\n",
    "        if not df_month.empty:\n",
    "            all_dfs.append(df_month)\n",
    "        # move to next month\n",
    "        next_month = (cur + timedelta(days=32)).replace(day=1)\n",
    "        cur = next_month\n",
    "    return pd.concat(all_dfs) if all_dfs else pd.DataFrame()\n",
    "\n",
    "# -----------------------\n",
    "# Run ingestion\n",
    "# -----------------------\n",
    "# Production 2022–2024\n",
    "df_prod = fetch_range_monthly(DATASET_PROD, PROD_START, PROD_END)\n",
    "print(\"Production data shape:\", df_prod.shape)\n",
    "print(df_prod.head())\n",
    "\n",
    "# Consumption 2021–2024\n",
    "df_cons = fetch_range_monthly(DATASET_CONS, CONS_START, CONS_END)\n",
    "print(\"Consumption data shape:\", df_cons.shape)\n",
    "print(df_cons.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f282abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production DataFrame ready: (657600, 4)\n",
      "Consumption DataFrame ready: (876600, 4)\n",
      "Production group names: ['hydro' 'wind' 'other' 'thermal' 'solar']\n",
      "Consumption group names: ['cabin' 'tertiary' 'household' 'secondary' 'primary']\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# FIX DATAFRAMES FOR SPARK / CASSANDRA\n",
    "# ============================\n",
    "\n",
    "# 1️⃣ Reset index so 'startTime' becomes a column\n",
    "df_prod = df_prod.reset_index()   # 'startTime' moves from index → column\n",
    "df_cons = df_cons.reset_index()\n",
    "\n",
    "# 2️⃣ Strip any extra whitespace from column names\n",
    "df_prod.columns = df_prod.columns.str.strip()\n",
    "df_cons.columns = df_cons.columns.str.strip()\n",
    "\n",
    "# 3️⃣ Ensure uniform group column name\n",
    "# (In your data, it's already 'groupName', but this covers any variations)\n",
    "if 'productionGroup' in df_prod.columns:\n",
    "    df_prod = df_prod.rename(columns={\"productionGroup\": \"groupName\"})\n",
    "if 'consumptionGroup' in df_cons.columns:\n",
    "    df_cons = df_cons.rename(columns={\"consumptionGroup\": \"groupName\"})\n",
    "\n",
    "# 4️⃣ Ensure proper types\n",
    "df_prod['startTime'] = pd.to_datetime(df_prod['startTime'], utc=True, errors='coerce')\n",
    "df_prod['quantityKwh'] = pd.to_numeric(df_prod['quantityKwh'], errors='coerce')\n",
    "\n",
    "df_cons['startTime'] = pd.to_datetime(df_cons['startTime'], utc=True, errors='coerce')\n",
    "df_cons['quantityKwh'] = pd.to_numeric(df_cons['quantityKwh'], errors='coerce')\n",
    "\n",
    "# 5️⃣ Drop rows with missing critical values\n",
    "df_prod = df_prod.dropna(subset=['startTime','priceArea','groupName','quantityKwh'])\n",
    "df_cons = df_cons.dropna(subset=['startTime','priceArea','groupName','quantityKwh'])\n",
    "\n",
    "# 6️⃣ Optional: drop 'endTime' if not needed in Cassandra\n",
    "df_prod = df_prod.drop(columns=['endTime'], errors='ignore')\n",
    "df_cons = df_cons.drop(columns=['endTime'], errors='ignore')\n",
    "\n",
    "# ✅ Check that DataFrames are ready\n",
    "print(\"Production DataFrame ready:\", df_prod.shape)\n",
    "print(\"Consumption DataFrame ready:\", df_cons.shape)\n",
    "print(\"Production group names:\", df_prod['groupName'].unique())\n",
    "print(\"Consumption group names:\", df_cons['groupName'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a515369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Keyspace and tables for production and consumption are ready\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "# Connect to local Cassandra\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "session = cluster.connect()\n",
    "\n",
    "# --- Create keyspace ---\n",
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS energy\n",
    "    WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\n",
    "\"\"\")\n",
    "\n",
    "# --- Create table for production ---\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS energy.production_per_group (\n",
    "        priceArea text,\n",
    "        startTime timestamp,\n",
    "        productionGroup text,\n",
    "        quantityKwh double,\n",
    "        PRIMARY KEY ((priceArea), startTime, productionGroup)\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# --- Create table for consumption ---\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS energy.consumption_per_group (\n",
    "        priceArea text,\n",
    "        startTime timestamp,\n",
    "        consumptionGroup text,\n",
    "        quantityKwh double,\n",
    "        PRIMARY KEY ((priceArea), startTime, consumptionGroup)\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Keyspace and tables for production and consumption are ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebbf7976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ startTime timezone fixed to UTC for both DataFrames\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Ensure startTime is timezone-aware UTC\n",
    "# -------------------------------\n",
    "\n",
    "import pytz\n",
    "\n",
    "# Production\n",
    "if df_prod['startTime'].dt.tz is None:\n",
    "    df_prod['startTime'] = df_prod['startTime'].dt.tz_localize('UTC')\n",
    "else:\n",
    "    df_prod['startTime'] = df_prod['startTime'].dt.tz_convert('UTC')\n",
    "\n",
    "# Consumption\n",
    "if df_cons['startTime'].dt.tz is None:\n",
    "    df_cons['startTime'] = df_cons['startTime'].dt.tz_localize('UTC')\n",
    "else:\n",
    "    df_cons['startTime'] = df_cons['startTime'].dt.tz_convert('UTC')\n",
    "\n",
    "print(\"✅ startTime timezone fixed to UTC for both DataFrames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ed07029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n",
      "Writing 657600 rows in 7 chunks for production...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 11:50:15 WARN TaskSetManager: Stage 0 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunk 1/7 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 11:50:18 WARN TaskSetManager: Stage 1 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunk 2/7 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 11:50:19 WARN TaskSetManager: Stage 2 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunk 3/7 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 11:50:21 WARN TaskSetManager: Stage 3 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunk 4/7 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 11:50:22 WARN TaskSetManager: Stage 4 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunk 5/7 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 11:50:24 WARN TaskSetManager: Stage 5 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunk 6/7 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 11:50:25 WARN TaskSetManager: Stage 6 contains a task of very large size (1993 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunk 7/7 written (57600 rows)\n",
      "Writing 876600 rows in 9 chunks for consumption...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 11:50:26 WARN TaskSetManager: Stage 7 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunk 1/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 11:50:28 WARN TaskSetManager: Stage 8 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunk 2/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 11:50:30 WARN TaskSetManager: Stage 9 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunk 3/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 11:50:31 WARN TaskSetManager: Stage 10 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunk 4/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 11:50:33 WARN TaskSetManager: Stage 11 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunk 5/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 11:50:34 WARN TaskSetManager: Stage 12 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunk 6/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 11:50:36 WARN TaskSetManager: Stage 13 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunk 7/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 11:50:38 WARN TaskSetManager: Stage 14 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunk 8/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 11:50:39 WARN TaskSetManager: Stage 15 contains a task of very large size (2826 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunk 9/9 written (76600 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType\n",
    "import math\n",
    "\n",
    "# --- CREATE SPARK SESSION ---\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ElhubDataIngest\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "# --- DEFINE COMMON SCHEMA ---\n",
    "schema = StructType([\n",
    "    StructField(\"priceArea\", StringType(), True),\n",
    "    StructField(\"groupName\", StringType(), True),\n",
    "    StructField(\"startTime\", TimestampType(), True),\n",
    "    StructField(\"quantityKwh\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# --- OPTIMIZED FUNCTION TO WRITE IN CHUNKS ---\n",
    "def write_to_cassandra_fast(df_pandas, dataset_type=\"production\", chunk_size=100_000):\n",
    "    if df_pandas is None or df_pandas.empty:\n",
    "        print(f\"No data to write for {dataset_type}\")\n",
    "        return\n",
    "\n",
    "    # Drop any leftover index columns\n",
    "    df_pandas = df_pandas.drop(columns=['index', 'level_0'], errors='ignore')\n",
    "\n",
    "    total_rows = len(df_pandas)\n",
    "    n_chunks = math.ceil(total_rows / chunk_size)\n",
    "    print(f\"Writing {total_rows} rows in {n_chunks} chunks for {dataset_type}...\")\n",
    "\n",
    "    for i in range(n_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i+1) * chunk_size, total_rows)\n",
    "        df_chunk = df_pandas.iloc[start_idx:end_idx]\n",
    "\n",
    "        # Make sure the columns match schema order exactly\n",
    "        df_chunk_spark = df_chunk[['priceArea', 'groupName', 'startTime', 'quantityKwh']]\n",
    "\n",
    "        # Convert to Spark DataFrame\n",
    "        spark_df = spark.createDataFrame(df_chunk_spark, schema=schema)\n",
    "\n",
    "        # Rename columns for Cassandra\n",
    "        if dataset_type == \"production\":\n",
    "            spark_df_cassandra = spark_df.selectExpr(\n",
    "                \"priceArea as pricearea\",\n",
    "                \"startTime as starttime\",\n",
    "                \"groupName as productiongroup\",\n",
    "                \"quantityKwh as quantitykwh\"\n",
    "            )\n",
    "            table = \"production_per_group\"\n",
    "        else:\n",
    "            spark_df_cassandra = spark_df.selectExpr(\n",
    "                \"priceArea as pricearea\",\n",
    "                \"startTime as starttime\",\n",
    "                \"groupName as consumptiongroup\",\n",
    "                \"quantityKwh as quantitykwh\"\n",
    "            )\n",
    "            table = \"consumption_per_group\"\n",
    "\n",
    "        # Reduce Spark partitions to 1 per chunk for faster small writes\n",
    "        spark_df_cassandra.coalesce(1).write \\\n",
    "            .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "            .options(keyspace=\"energy\", table=table) \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "\n",
    "        print(f\"✅ Chunk {i+1}/{n_chunks} written ({len(df_chunk)} rows)\")\n",
    "\n",
    "# --- WRITE BOTH DATASETS ---\n",
    "write_to_cassandra_fast(df_prod, \"production\")\n",
    "write_to_cassandra_fast(df_cons, \"consumption\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4321e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Production table preview:\n",
      "+---------+-------------------+---------------+-----------+\n",
      "|pricearea|starttime          |productiongroup|quantitykwh|\n",
      "+---------+-------------------+---------------+-----------+\n",
      "|NO4      |2021-01-01 00:00:00|hydro          |3740830.0  |\n",
      "|NO4      |2021-01-01 00:00:00|other          |0.161      |\n",
      "|NO4      |2021-01-01 00:00:00|solar          |0.0        |\n",
      "|NO4      |2021-01-01 00:00:00|thermal        |21349.0    |\n",
      "|NO4      |2021-01-01 00:00:00|wind           |381065.0   |\n",
      "+---------+-------------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in production_per_group: 872853\n",
      "\n",
      "✅ Consumption table preview:\n",
      "+---------+-------------------+----------------+-----------+\n",
      "|pricearea|starttime          |consumptiongroup|quantitykwh|\n",
      "+---------+-------------------+----------------+-----------+\n",
      "|NO4      |2021-01-01 00:00:00|cabin           |56058.316  |\n",
      "|NO4      |2021-01-01 00:00:00|household       |675104.9   |\n",
      "|NO4      |2021-01-01 00:00:00|primary         |51439.465  |\n",
      "|NO4      |2021-01-01 00:00:00|secondary       |848937.56  |\n",
      "|NO4      |2021-01-01 00:00:00|tertiary        |366046.0   |\n",
      "+---------+-------------------+----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows in consumption_per_group: 876500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Production groups in Cassandra: [Row(productiongroup='solar'), Row(productiongroup='other'), Row(productiongroup='thermal'), Row(productiongroup='hydro'), Row(productiongroup='wind')]\n",
      "Consumption groups in Cassandra: [Row(consumptiongroup='tertiary'), Row(consumptiongroup='secondary'), Row(consumptiongroup='household'), Row(consumptiongroup='cabin'), Row(consumptiongroup='primary')]\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3️⃣ READ DATA BACK FROM CASSANDRA\n",
    "# ==========================================\n",
    "def read_from_cassandra(table_name):\n",
    "    \"\"\"Reads a table from the 'energy' keyspace into a Spark DataFrame.\"\"\"\n",
    "    df = spark.read \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(keyspace=\"energy\", table=table_name) \\\n",
    "        .load()\n",
    "    return df\n",
    "\n",
    "# --- Read production table ---\n",
    "cass_prod_df = read_from_cassandra(\"production_per_group\")\n",
    "print(\"✅ Production table preview:\")\n",
    "cass_prod_df.show(5, truncate=False)\n",
    "\n",
    "prod_count = cass_prod_df.count()\n",
    "print(f\"Total rows in production_per_group: {prod_count}\")\n",
    "\n",
    "# --- Read consumption table ---\n",
    "cass_cons_df = read_from_cassandra(\"consumption_per_group\")\n",
    "print(\"\\n✅ Consumption table preview:\")\n",
    "cass_cons_df.show(5, truncate=False)\n",
    "\n",
    "cons_count = cass_cons_df.count()\n",
    "print(f\"Total rows in consumption_per_group: {cons_count}\")\n",
    "\n",
    "# --- Optional: basic sanity check on groups ---\n",
    "print(\"\\nProduction groups in Cassandra:\", cass_prod_df.select(\"productiongroup\").distinct().collect())\n",
    "print(\"Consumption groups in Cassandra:\", cass_cons_df.select(\"consumptiongroup\").distinct().collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8242259a",
   "metadata": {},
   "source": [
    "### Elhub data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b9559a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b0469ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n",
      "✅ Cassandra keyspace and tables ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 10:45:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Fetch and store Elhub production & consumption hourly data (2021-2024)\n",
    "# Adapted from your earlier notebook\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import tz\n",
    "import pytz\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Cassandra / Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType\n",
    "\n",
    "# MongoDB\n",
    "from pymongo import MongoClient, ASCENDING\n",
    "from pymongo.errors import BulkWriteError, DuplicateKeyError\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "BASE_URL = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "# datasets\n",
    "DATASET_PROD = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "DATASET_CONS = \"CONSUMPTION_PER_GROUP_MBA_HOUR\"\n",
    "\n",
    "# Date ranges\n",
    "PROD_START = datetime(2022, 1, 1)\n",
    "PROD_END   = datetime(2024, 12, 31, 23, 59, 59)\n",
    "\n",
    "CONS_START = datetime(2021, 1, 1)\n",
    "CONS_END   = datetime(2024, 12, 31, 23, 59, 59)\n",
    "\n",
    "# API timezone formatting: your previous code used +02:00 encoded as %2B02:00\n",
    "def format_date(dt_obj):\n",
    "    \"\"\"Format datetime in the Elhub expected format with +02:00 percent-encoded.\n",
    "    We send naive datetimes in local CET/CEST? To be consistent with your earlier code,\n",
    "    we format as YYYY-MM-DDTHH:MM:SS%2B02:00\n",
    "    \"\"\"\n",
    "    return dt_obj.strftime(\"%Y-%m-%dT%H:%M:%S%%2B02:00\")  # +02:00 encoded\n",
    "\n",
    "HEADERS = {\n",
    "    \"Accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Networking / retry config\n",
    "MAX_RETRIES = 5\n",
    "SLEEP_BETWEEN_REQUESTS = 0.6  # gentle\n",
    "BACKOFF_FACTOR = 2.0\n",
    "\n",
    "# Local DB hosts\n",
    "CASSANDRA_HOST = \"127.0.0.1\"\n",
    "MONGO_URI = \"mongodb://127.0.0.1:27017\"\n",
    "MONGO_DBNAME = \"energy\"\n",
    "\n",
    "# -----------------------\n",
    "# Helper: request with retry\n",
    "# -----------------------\n",
    "def get_with_retry(url, max_retries=MAX_RETRIES):\n",
    "    wait = 1.0\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=60)\n",
    "            if r.status_code == 200:\n",
    "                return r.json()\n",
    "            else:\n",
    "                print(f\"Warning: status {r.status_code} for {url}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"RequestException: {e} (attempt {attempt}/{max_retries})\")\n",
    "        if attempt < max_retries:\n",
    "            time.sleep(wait)\n",
    "            wait *= BACKOFF_FACTOR\n",
    "    raise RuntimeError(f\"Failed to fetch {url} after {max_retries} attempts\")\n",
    "\n",
    "# -----------------------\n",
    "# Extract records from response\n",
    "# -----------------------\n",
    "def extract_records_from_response(response_json, dataset):\n",
    "    \"\"\"Return list of dict records with fields: priceArea, group, startTime, quantityKwh\"\"\"\n",
    "    data = response_json.get(\"data\", [])\n",
    "    all_recs = []\n",
    "    if dataset == DATASET_PROD:\n",
    "        key = \"productionPerGroupMbaHour\"\n",
    "        group_field = \"productionGroup\"\n",
    "    elif dataset == DATASET_CONS:\n",
    "        key = \"consumptionPerGroupMbaHour\"\n",
    "        group_field = \"consumptionGroup\"\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset\")\n",
    "\n",
    "    for entry in data:\n",
    "        attrs = entry.get(\"attributes\", {})\n",
    "        recs = attrs.get(key, [])\n",
    "        # Filter placeholder groups\n",
    "        for r in recs:\n",
    "            if r.get(group_field) == \"*\" or r.get(group_field) is None:\n",
    "                continue\n",
    "            # Build normalized record\n",
    "            rec = {\n",
    "                \"priceArea\": r.get(\"priceArea\"),\n",
    "                \"group\": r.get(group_field),\n",
    "                \"startTime\": r.get(\"startTime\"),\n",
    "                \"endTime\": r.get(\"endTime\"),\n",
    "                \"quantityKwh\": r.get(\"quantityKwh\")\n",
    "            }\n",
    "            all_recs.append(rec)\n",
    "    return all_recs\n",
    "\n",
    "# -----------------------\n",
    "# Fetch in daily chunks (safer than monthly)\n",
    "# -----------------------\n",
    "def fetch_range_daily(dataset, start_dt, end_dt, sleep=SLEEP_BETWEEN_REQUESTS):\n",
    "    \"\"\"Yield DataFrame chunks for each day in [start_dt, end_dt].\"\"\"\n",
    "    records = []\n",
    "    cur = start_dt\n",
    "    # ensure start at midnight\n",
    "    cur = cur.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    end_limit = end_dt\n",
    "    while cur <= end_limit:\n",
    "        day_start = cur\n",
    "        day_end = cur + timedelta(days=1) - timedelta(seconds=1)\n",
    "        start_str = format_date(day_start)\n",
    "        end_str = format_date(day_end)\n",
    "        url = f\"{BASE_URL}?dataset={dataset}&startDate={start_str}&endDate={end_str}\"\n",
    "        # print progress per day\n",
    "        try:\n",
    "            resp = get_with_retry(url)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR fetching {day_start.date()}: {e}\")\n",
    "            cur += timedelta(days=1)\n",
    "            time.sleep(sleep)\n",
    "            continue\n",
    "\n",
    "        recs = extract_records_from_response(resp, dataset)\n",
    "        if recs:\n",
    "            df = pd.DataFrame(recs)\n",
    "            # parse times; Elhub uses timezone suffix; parse with pandas\n",
    "            df['startTime'] = pd.to_datetime(df['startTime'], utc=True, errors='coerce')\n",
    "            df['endTime'] = pd.to_datetime(df['endTime'], utc=True, errors='coerce')\n",
    "            df['quantityKwh'] = pd.to_numeric(df['quantityKwh'], errors='coerce')\n",
    "            df = df.rename(columns={\"group\": \"groupName\"})\n",
    "            # normalize column names\n",
    "            df = df[['priceArea', 'groupName', 'startTime', 'endTime', 'quantityKwh']]\n",
    "            yield df\n",
    "        # be kind to API\n",
    "        time.sleep(sleep)\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "# -----------------------\n",
    "# Spark session (for Cassandra writes)\n",
    "# -----------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ElhubDataIngest\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", CASSANDRA_HOST) \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "# Ensure Cassandra keyspace and tables exist\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster([CASSANDRA_HOST])\n",
    "cass_session = cluster.connect()\n",
    "cass_session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS energy\n",
    "    WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\n",
    "\"\"\")\n",
    "\n",
    "# Production table\n",
    "cass_session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS energy.production_per_group (\n",
    "        priceArea text,\n",
    "        startTime timestamp,\n",
    "        productionGroup text,\n",
    "        quantityKwh double,\n",
    "        PRIMARY KEY ((priceArea), startTime, productionGroup)\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# Consumption table\n",
    "cass_session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS energy.consumption_per_group (\n",
    "        priceArea text,\n",
    "        startTime timestamp,\n",
    "        consumptionGroup text,\n",
    "        quantityKwh double,\n",
    "        PRIMARY KEY ((priceArea), startTime, consumptionGroup)\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Cassandra keyspace and tables ready\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0458e23",
   "metadata": {},
   "outputs": [
    {
     "ename": "ServerSelectionTimeoutError",
     "evalue": "127.0.0.1:27017: [Errno 61] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 691305deffb406b1a4413861, topology_type: Unknown, servers: [<ServerDescription ('127.0.0.1', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('127.0.0.1:27017: [Errno 61] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mServerSelectionTimeoutError\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m cons_col = mdb[\u001b[33m\"\u001b[39m\u001b[33mconsumption_per_group\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# create unique indexes to prevent duplicates on (priceArea, startTime, groupName)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mprod_col\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpriceArea\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mASCENDING\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstartTime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mASCENDING\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgroupName\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mASCENDING\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43munique\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprod_unique_idx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m cons_col.create_index(\n\u001b[32m     16\u001b[39m     [(\u001b[33m\"\u001b[39m\u001b[33mpriceArea\u001b[39m\u001b[33m\"\u001b[39m, ASCENDING), (\u001b[33m\"\u001b[39m\u001b[33mstartTime\u001b[39m\u001b[33m\"\u001b[39m, ASCENDING), (\u001b[33m\"\u001b[39m\u001b[33mgroupName\u001b[39m\u001b[33m\"\u001b[39m, ASCENDING)],\n\u001b[32m     17\u001b[39m     unique=\u001b[38;5;28;01mTrue\u001b[39;00m, name=\u001b[33m\"\u001b[39m\u001b[33mcons_unique_idx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ MongoDB indexes created/ensured\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/D2D_env/lib/python3.11/site-packages/pymongo/synchronous/collection.py:2387\u001b[39m, in \u001b[36mCollection.create_index\u001b[39m\u001b[34m(self, keys, session, comment, **kwargs)\u001b[39m\n\u001b[32m   2385\u001b[39m     cmd_options[\u001b[33m\"\u001b[39m\u001b[33mcomment\u001b[39m\u001b[33m\"\u001b[39m] = comment\n\u001b[32m   2386\u001b[39m index = IndexModel(keys, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2387\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcmd_options\u001b[49m\u001b[43m)\u001b[49m)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/D2D_env/lib/python3.11/site-packages/pymongo/_csot.py:125\u001b[39m, in \u001b[36mapply.<locals>.csot_wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/D2D_env/lib/python3.11/site-packages/pymongo/synchronous/collection.py:2240\u001b[39m, in \u001b[36mCollection._create_indexes\u001b[39m\u001b[34m(self, indexes, session, **kwargs)\u001b[39m\n\u001b[32m   2230\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Internal createIndexes helper.\u001b[39;00m\n\u001b[32m   2231\u001b[39m \n\u001b[32m   2232\u001b[39m \u001b[33;03m:param indexes: A list of :class:`~pymongo.operations.IndexModel`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2237\u001b[39m \u001b[33;03m    command (like maxTimeMS) can be passed as keyword arguments.\u001b[39;00m\n\u001b[32m   2238\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2239\u001b[39m names = []\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conn_for_writes\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_Op\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCREATE_INDEXES\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[32m   2241\u001b[39m     supports_quorum = conn.max_wire_version >= \u001b[32m9\u001b[39m\n\u001b[32m   2243\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgen_indexes\u001b[39m() -> Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/D2D_env/lib/python3.11/site-packages/pymongo/synchronous/collection.py:578\u001b[39m, in \u001b[36mCollection._conn_for_writes\u001b[39m\u001b[34m(self, session, operation)\u001b[39m\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_conn_for_writes\u001b[39m(\n\u001b[32m    576\u001b[39m     \u001b[38;5;28mself\u001b[39m, session: Optional[ClientSession], operation: \u001b[38;5;28mstr\u001b[39m\n\u001b[32m    577\u001b[39m ) -> ContextManager[Connection]:\n\u001b[32m--> \u001b[39m\u001b[32m578\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_database\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_conn_for_writes\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/D2D_env/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py:1850\u001b[39m, in \u001b[36mMongoClient._conn_for_writes\u001b[39m\u001b[34m(self, session, operation)\u001b[39m\n\u001b[32m   1847\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_conn_for_writes\u001b[39m(\n\u001b[32m   1848\u001b[39m     \u001b[38;5;28mself\u001b[39m, session: Optional[ClientSession], operation: \u001b[38;5;28mstr\u001b[39m\n\u001b[32m   1849\u001b[39m ) -> ContextManager[Connection]:\n\u001b[32m-> \u001b[39m\u001b[32m1850\u001b[39m     server = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwritable_server_selector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1851\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._checkout(server, session)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/D2D_env/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py:1833\u001b[39m, in \u001b[36mMongoClient._select_server\u001b[39m\u001b[34m(self, server_selector, session, operation, address, deprioritized_servers, operation_id)\u001b[39m\n\u001b[32m   1831\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m AutoReconnect(\u001b[33m\"\u001b[39m\u001b[33mserver \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m no longer available\u001b[39m\u001b[33m\"\u001b[39m % address)  \u001b[38;5;66;03m# noqa: UP031\u001b[39;00m\n\u001b[32m   1832\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1833\u001b[39m         server = \u001b[43mtopology\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1834\u001b[39m \u001b[43m            \u001b[49m\u001b[43mserver_selector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1835\u001b[39m \u001b[43m            \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1836\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdeprioritized_servers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeprioritized_servers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1837\u001b[39m \u001b[43m            \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1838\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1839\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m server\n\u001b[32m   1840\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PyMongoError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m   1841\u001b[39m     \u001b[38;5;66;03m# Server selection errors in a transaction are transient.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/D2D_env/lib/python3.11/site-packages/pymongo/synchronous/topology.py:409\u001b[39m, in \u001b[36mTopology.select_server\u001b[39m\u001b[34m(self, selector, operation, server_selection_timeout, address, deprioritized_servers, operation_id)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect_server\u001b[39m(\n\u001b[32m    400\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    401\u001b[39m     selector: Callable[[Selection], Selection],\n\u001b[32m   (...)\u001b[39m\u001b[32m    406\u001b[39m     operation_id: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    407\u001b[39m ) -> Server:\n\u001b[32m    408\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Like select_servers, but choose a random server if several match.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     server = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_selection_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeprioritized_servers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _csot.get_timeout():\n\u001b[32m    418\u001b[39m         _csot.set_rtt(server.description.min_round_trip_time)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/D2D_env/lib/python3.11/site-packages/pymongo/synchronous/topology.py:387\u001b[39m, in \u001b[36mTopology._select_server\u001b[39m\u001b[34m(self, selector, operation, server_selection_timeout, address, deprioritized_servers, operation_id)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_select_server\u001b[39m(\n\u001b[32m    379\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    380\u001b[39m     selector: Callable[[Selection], Selection],\n\u001b[32m   (...)\u001b[39m\u001b[32m    385\u001b[39m     operation_id: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    386\u001b[39m ) -> Server:\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m     servers = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mselect_servers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_selection_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_id\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m     servers = _filter_servers(servers, deprioritized_servers)\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(servers) == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/D2D_env/lib/python3.11/site-packages/pymongo/synchronous/topology.py:294\u001b[39m, in \u001b[36mTopology.select_servers\u001b[39m\u001b[34m(self, selector, operation, server_selection_timeout, address, operation_id)\u001b[39m\n\u001b[32m    291\u001b[39m     \u001b[38;5;28mself\u001b[39m.cleanup_monitors()\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     server_descriptions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select_servers_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddress\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    299\u001b[39m         cast(Server, \u001b[38;5;28mself\u001b[39m.get_server_by_address(sd.address)) \u001b[38;5;28;01mfor\u001b[39;00m sd \u001b[38;5;129;01min\u001b[39;00m server_descriptions\n\u001b[32m    300\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/D2D_env/lib/python3.11/site-packages/pymongo/synchronous/topology.py:344\u001b[39m, in \u001b[36mTopology._select_servers_loop\u001b[39m\u001b[34m(self, selector, timeout, operation, operation_id, address)\u001b[39m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _SERVER_SELECTION_LOGGER.isEnabledFor(logging.DEBUG):\n\u001b[32m    334\u001b[39m         _debug_log(\n\u001b[32m    335\u001b[39m             _SERVER_SELECTION_LOGGER,\n\u001b[32m    336\u001b[39m             message=_ServerSelectionStatusMessage.FAILED,\n\u001b[32m   (...)\u001b[39m\u001b[32m    342\u001b[39m             failure=\u001b[38;5;28mself\u001b[39m._error_message(selector),\n\u001b[32m    343\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServerSelectionTimeoutError(\n\u001b[32m    345\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._error_message(selector)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Timeout: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms, Topology Description: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.description\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    346\u001b[39m     )\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logged_waiting:\n\u001b[32m    349\u001b[39m     _debug_log(\n\u001b[32m    350\u001b[39m         _SERVER_SELECTION_LOGGER,\n\u001b[32m    351\u001b[39m         message=_ServerSelectionStatusMessage.WAITING,\n\u001b[32m   (...)\u001b[39m\u001b[32m    357\u001b[39m         remainingTimeMS=\u001b[38;5;28mint\u001b[39m(\u001b[32m1000\u001b[39m * (end_time - time.monotonic())),\n\u001b[32m    358\u001b[39m     )\n",
      "\u001b[31mServerSelectionTimeoutError\u001b[39m: 127.0.0.1:27017: [Errno 61] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 691305deffb406b1a4413861, topology_type: Unknown, servers: [<ServerDescription ('127.0.0.1', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('127.0.0.1:27017: [Errno 61] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# MongoDB setup\n",
    "# -----------------------\n",
    "mongo_client = MongoClient(MONGO_URI)\n",
    "mdb = mongo_client[MONGO_DBNAME]\n",
    "prod_col = mdb[\"production_per_group\"]\n",
    "cons_col = mdb[\"consumption_per_group\"]\n",
    "\n",
    "\n",
    "# create unique indexes to prevent duplicates on (priceArea, startTime, groupName)\n",
    "prod_col.create_index(\n",
    "    [(\"priceArea\", ASCENDING), (\"startTime\", ASCENDING), (\"groupName\", ASCENDING)],\n",
    "    unique=True, name=\"prod_unique_idx\"\n",
    ")\n",
    "cons_col.create_index(\n",
    "    [(\"priceArea\", ASCENDING), (\"startTime\", ASCENDING), (\"groupName\", ASCENDING)],\n",
    "    unique=True, name=\"cons_unique_idx\"\n",
    ")\n",
    "print(\"✅ MongoDB indexes created/ensured\")\n",
    "\n",
    "# -----------------------\n",
    "# Common schema for Spark DF creation\n",
    "# -----------------------\n",
    "schema = StructType([\n",
    "    StructField(\"priceArea\", StringType(), True),\n",
    "    StructField(\"groupName\", StringType(), True),\n",
    "    StructField(\"startTime\", TimestampType(), True),\n",
    "    StructField(\"endTime\", TimestampType(), True),\n",
    "    StructField(\"quantityKwh\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Helper to write pandas df to Cassandra using Spark\n",
    "def write_to_cassandra(df_pandas, dataset_type=\"production\"):\n",
    "    if df_pandas is None or df_pandas.empty:\n",
    "        return\n",
    "    # Convert pandas -> spark\n",
    "    spark_df = spark.createDataFrame(df_pandas.astype(object), schema=schema)\n",
    "    # rename columns per Cassandra table\n",
    "    if dataset_type == \"production\":\n",
    "        spark_df_cassandra = spark_df.selectExpr(\n",
    "            \"priceArea as pricearea\",\n",
    "            \"startTime as starttime\",\n",
    "            \"groupName as productiongroup\",\n",
    "            \"quantityKwh as quantitykwh\"\n",
    "        )\n",
    "        keyspace = \"energy\"\n",
    "        table = \"production_per_group\"\n",
    "    else:\n",
    "        spark_df_cassandra = spark_df.selectExpr(\n",
    "            \"priceArea as pricearea\",\n",
    "            \"startTime as starttime\",\n",
    "            \"groupName as consumptiongroup\",\n",
    "            \"quantityKwh as quantitykwh\"\n",
    "        )\n",
    "        keyspace = \"energy\"\n",
    "        table = \"consumption_per_group\"\n",
    "\n",
    "    # write in append mode\n",
    "    spark_df_cassandra.write \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(keyspace=keyspace, table=table) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "# Helper to write to MongoDB (bulk upsert-ish by ignoring duplicates)\n",
    "def write_to_mongo(df_pandas, collection):\n",
    "    if df_pandas is None or df_pandas.empty:\n",
    "        return\n",
    "    # prepare bulk inserts (Mongo will reject duplicates due to unique index)\n",
    "    docs = []\n",
    "    for _, r in df_pandas.iterrows():\n",
    "        doc = {\n",
    "            \"priceArea\": r['priceArea'],\n",
    "            \"groupName\": r['groupName'],\n",
    "            \"startTime\": pd.to_datetime(r['startTime']).to_pydatetime(),\n",
    "            \"endTime\": pd.to_datetime(r['endTime']).to_pydatetime(),\n",
    "            \"quantityKwh\": None if pd.isna(r['quantityKwh']) else float(r['quantityKwh'])\n",
    "        }\n",
    "        docs.append(doc)\n",
    "    if not docs:\n",
    "        return\n",
    "    try:\n",
    "        collection.insert_many(docs, ordered=False)\n",
    "    except BulkWriteError as bwe:\n",
    "        # most likely duplicate key errors; ignore and continue\n",
    "        print(\"Mongo BulkWriteError (likely duplicates). Inserted some documents; duplicates skipped.\")\n",
    "    except DuplicateKeyError:\n",
    "        print(\"Mongo DuplicateKeyError (skipped)\")\n",
    "\n",
    "# -----------------------\n",
    "# Ingest loops\n",
    "# -----------------------\n",
    "def ingest_dataset(dataset, start_dt, end_dt, target_mongo_collection, dataset_type):\n",
    "    total_rows = 0\n",
    "    day_count = (end_dt.date() - start_dt.date()).days + 1\n",
    "    print(f\"Starting ingest for {dataset} from {start_dt.date()} to {end_dt.date()} ({day_count} days)\")\n",
    "    for df_day in tqdm(fetch_range_daily(dataset, start_dt, end_dt), desc=f\"Ingesting {dataset_type}\"):\n",
    "        # df_day has columns: priceArea, groupName, startTime, endTime, quantityKwh\n",
    "        if df_day.empty:\n",
    "            continue\n",
    "        # drop rows with null key fields\n",
    "        df_day = df_day.dropna(subset=['priceArea', 'groupName', 'startTime'])\n",
    "        if df_day.empty:\n",
    "            continue\n",
    "\n",
    "        # write to MongoDB\n",
    "        write_to_mongo(df_day, target_mongo_collection)\n",
    "\n",
    "        # write to Cassandra via Spark\n",
    "        # Note: our Cassandra table doesn't include endTime: we create Spark df with endTime in schema but only map the needed columns\n",
    "        write_to_cassandra(df_day, dataset_type)\n",
    "\n",
    "        total_rows += len(df_day)\n",
    "    print(f\"Finished ingest for {dataset_type}. Total rows processed: {total_rows}\")\n",
    "\n",
    "# Run production ingest (2022-2024)\n",
    "ingest_dataset(DATASET_PROD, PROD_START, PROD_END, prod_col, dataset_type=\"production\")\n",
    "\n",
    "# Run consumption ingest (2021-2024)\n",
    "ingest_dataset(DATASET_CONS, CONS_START, CONS_END, cons_col, dataset_type=\"consumption\")\n",
    "\n",
    "# -----------------------\n",
    "# Basic verification prints\n",
    "# -----------------------\n",
    "print(\"Mongo counts (approx):\")\n",
    "print(\"production:\", prod_col.count_documents({}))\n",
    "print(\"consumption:\", cons_col.count_documents({}))\n",
    "\n",
    "# Spark read-back sample (production)\n",
    "print(\"Sample rows from Cassandra production_per_group:\")\n",
    "cass_prod = spark.read.format(\"org.apache.spark.sql.cassandra\").options(keyspace=\"energy\", table=\"production_per_group\").load()\n",
    "cass_prod.show(5)\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e35d2957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 10:44:45 WARN Utils: Your hostname, Sara-sin-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 172.28.28.196 instead (on interface en0)\n",
      "25/11/11 10:44:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/11 10:44:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java version (Spark): 17.0.17\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"CheckJava\").getOrCreate()\n",
    "print(\"Java version (Spark):\", spark.sparkContext._gateway.jvm.java.lang.System.getProperty(\"java.version\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "427b7739",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ea60a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Java/JavaVirtualMachines/microsoft-11.jdk/Contents/Home\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get(\"JAVA_HOME\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2D_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
