{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb9ffc5",
   "metadata": {},
   "source": [
    "# Project work, part 4\n",
    "Sara H√∏rte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4686130d",
   "metadata": {},
   "source": [
    "### Links\n",
    "\n",
    "GitHub repo link: \n",
    "https://github.com/sarahorte/ind320project.git\n",
    "\n",
    "Streamlit app link: \n",
    "https://ind320project.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7c1b09",
   "metadata": {},
   "source": [
    "### Bonus exercise: \n",
    "Snow drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fceb9b3",
   "metadata": {},
   "source": [
    "### Log\n",
    "\n",
    "Hente data 2021-2024. slette det som er i mongo fra f√∏r for √• ha nok plass. gj√∏r det f√∏rst. se installation hvs jeg sliter.\n",
    "ogs√• energy consumption.\n",
    "Bytt Java 17 hvis problemer!\n",
    "\n",
    "oppdatere plottene til plotly. tror spectrogram er eneste som mangler det.\n",
    "\n",
    "map osv\n",
    "- use choroplethmap. se pladlet\n",
    "- session 1 17:00 10.nov\n",
    "\n",
    "\n",
    "snow\n",
    "-mulig wind drift ikke er dynamisk, og derfor ikke plotly\n",
    "tror snow er den enkleste oppgaven. \n",
    "\n",
    "\n",
    "\n",
    "organisere sidene.\n",
    "\n",
    "\n",
    "\n",
    "the first thing i did was to fetch the data from 2022-2024 for produciton and 2021-2024 for consumption. i struggled a bit with the connections and getting the databases correct, but not too much. i unfortunaltely managed to delete the already existiting data from 2021 in mongoDB, so i had to go back to CA2 to load this again. then readjusting the code for the new data to not clear the table before loading new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed753b8e",
   "metadata": {},
   "source": [
    "### AI usage\n",
    "AI was used extensively throughout this project, primarily through ChatGPT and GitHub Copilot in VS Code. I leveraged AI to generate code suggestions, adapt examples from lectures, and refine solutions for specific tasks. For instance, when implementing the Local Outlier Factor method, I combined code snippets from lectures with AI-generated suggestions, then iteratively adjusted and tested the code until it functioned correctly.\n",
    "\n",
    "I also used AI to interpret error messages and propose potential fixes. By testing different solutions suggested by ChatGPT, I was able to identify the most effective approach. Additionally, AI assisted in generating initial visualizations, which I later fine-tuned to achieve the desired appearance. For parameter selection in the various analysis functions, ChatGPT provided guidance, which was helpful as a starting point, though understanding the underlying concepts is essential for making meaningful choices.\n",
    "\n",
    "The interactive nature of the Streamlit app made it easy to experiment with different parameters, and AI guidance accelerated this process. Beyond coding, I also used ChatGPT to refine and improve the wording of both the project log and the AI usage section itself, ensuring clarity and readability.\n",
    "\n",
    "Overall, AI proved to be a valuable tool for both code development and documentation, complementing my own understanding and enabling more efficient problem-solving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ec980",
   "metadata": {},
   "source": [
    "### Elhub data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eded1006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production data shape: (657600, 4)\n",
      "                          priceArea groupName                   endTime  \\\n",
      "startTime                                                                 \n",
      "2021-12-31 23:00:00+00:00       NO1     hydro 2022-01-01 00:00:00+00:00   \n",
      "2021-12-31 23:00:00+00:00       NO4      wind 2022-01-01 00:00:00+00:00   \n",
      "2021-12-31 23:00:00+00:00       NO2     other 2022-01-01 00:00:00+00:00   \n",
      "2021-12-31 23:00:00+00:00       NO4   thermal 2022-01-01 00:00:00+00:00   \n",
      "2021-12-31 23:00:00+00:00       NO1   thermal 2022-01-01 00:00:00+00:00   \n",
      "\n",
      "                           quantityKwh  \n",
      "startTime                               \n",
      "2021-12-31 23:00:00+00:00   1291422.40  \n",
      "2021-12-31 23:00:00+00:00    320912.40  \n",
      "2021-12-31 23:00:00+00:00         0.20  \n",
      "2021-12-31 23:00:00+00:00     38372.56  \n",
      "2021-12-31 23:00:00+00:00     30049.92  \n",
      "Consumption data shape: (876600, 4)\n",
      "                          priceArea  groupName                   endTime  \\\n",
      "startTime                                                                  \n",
      "2020-12-31 23:00:00+00:00       NO1      cabin 2021-01-01 00:00:00+00:00   \n",
      "2020-12-31 23:00:00+00:00       NO4   tertiary 2021-01-01 00:00:00+00:00   \n",
      "2020-12-31 23:00:00+00:00       NO2  household 2021-01-01 00:00:00+00:00   \n",
      "2020-12-31 23:00:00+00:00       NO4  secondary 2021-01-01 00:00:00+00:00   \n",
      "2020-12-31 23:00:00+00:00       NO1  secondary 2021-01-01 00:00:00+00:00   \n",
      "\n",
      "                           quantityKwh  \n",
      "startTime                               \n",
      "2020-12-31 23:00:00+00:00    177071.56  \n",
      "2020-12-31 23:00:00+00:00    366046.00  \n",
      "2020-12-31 23:00:00+00:00   1425141.00  \n",
      "2020-12-31 23:00:00+00:00    848937.56  \n",
      "2020-12-31 23:00:00+00:00    693011.75  \n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# FETCH DATA FROM ELHUB API (2021-2024) ‚Äì FASTER\n",
    "# =============================\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- API SETTINGS ---\n",
    "BASE_URL = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "\n",
    "# datasets\n",
    "DATASET_PROD = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "DATASET_CONS = \"CONSUMPTION_PER_GROUP_MBA_HOUR\"\n",
    "\n",
    "# --- DATE RANGES ---\n",
    "PROD_START = datetime(2022, 1, 1)\n",
    "PROD_END   = datetime(2024, 12, 31)\n",
    "\n",
    "CONS_START = datetime(2021, 1, 1)\n",
    "CONS_END   = datetime(2024, 12, 31)\n",
    "\n",
    "# --- FUNCTION TO FORMAT DATES ---\n",
    "def format_date(dt_obj):\n",
    "    \"\"\"Formats datetime for Elhub API (+02:00 offset).\"\"\"\n",
    "    return dt_obj.strftime(\"%Y-%m-%dT%H:%M:%S%%2B02:00\")  # +02:00 encoded\n",
    "\n",
    "HEADERS = {\"Accept\": \"application/json\"}\n",
    "\n",
    "# -----------------------\n",
    "# Fetch data for a given month\n",
    "# -----------------------\n",
    "def fetch_month(dataset, year, month):\n",
    "    start = datetime(year, month, 1)\n",
    "    next_month = (start + timedelta(days=32)).replace(day=1)\n",
    "    end = next_month - timedelta(seconds=1)\n",
    "\n",
    "    url = f\"{BASE_URL}?dataset={dataset}&startDate={format_date(start)}&endDate={format_date(end)}\"\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=60)\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"‚ùå Error {resp.status_code} for {start.date()} ‚Üí {end.date()}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    data = resp.json().get(\"data\", [])\n",
    "    records = []\n",
    "    for entry in data:\n",
    "        attrs = entry.get(\"attributes\", {})\n",
    "        if dataset == DATASET_PROD:\n",
    "            key = \"productionPerGroupMbaHour\"\n",
    "            group_field = \"productionGroup\"\n",
    "        else:\n",
    "            key = \"consumptionPerGroupMbaHour\"\n",
    "            group_field = \"consumptionGroup\"\n",
    "        recs = attrs.get(key, [])\n",
    "        recs = [r for r in recs if r.get(group_field) != \"*\" and r.get(group_field) is not None]\n",
    "        for r in recs:\n",
    "            records.append({\n",
    "                \"priceArea\": r.get(\"priceArea\"),\n",
    "                \"groupName\": r.get(group_field),\n",
    "                \"startTime\": r.get(\"startTime\"),\n",
    "                \"endTime\": r.get(\"endTime\"),\n",
    "                \"quantityKwh\": r.get(\"quantityKwh\")\n",
    "            })\n",
    "    if records:\n",
    "        df = pd.DataFrame(records)\n",
    "        df['startTime'] = pd.to_datetime(df['startTime'], utc=True, errors='coerce')\n",
    "        df['endTime'] = pd.to_datetime(df['endTime'], utc=True, errors='coerce')\n",
    "        df['quantityKwh'] = pd.to_numeric(df['quantityKwh'], errors='coerce')\n",
    "        df = df[['priceArea', 'groupName', 'startTime', 'endTime', 'quantityKwh']]\n",
    "        df.sort_values('startTime', inplace=True)\n",
    "        df.set_index('startTime', inplace=True)\n",
    "        return df\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# -----------------------\n",
    "# Fetch all months in a range\n",
    "# -----------------------\n",
    "def fetch_range_monthly(dataset, start_dt, end_dt):\n",
    "    all_dfs = []\n",
    "    cur = start_dt\n",
    "    while cur <= end_dt:\n",
    "        df_month = fetch_month(dataset, cur.year, cur.month)\n",
    "        if not df_month.empty:\n",
    "            all_dfs.append(df_month)\n",
    "        # move to next month\n",
    "        next_month = (cur + timedelta(days=32)).replace(day=1)\n",
    "        cur = next_month\n",
    "    return pd.concat(all_dfs) if all_dfs else pd.DataFrame()\n",
    "\n",
    "# -----------------------\n",
    "# Run ingestion\n",
    "# -----------------------\n",
    "# Production 2022‚Äì2024\n",
    "df_prod = fetch_range_monthly(DATASET_PROD, PROD_START, PROD_END)\n",
    "print(\"Production data shape:\", df_prod.shape)\n",
    "print(df_prod.head())\n",
    "\n",
    "# Consumption 2021‚Äì2024\n",
    "df_cons = fetch_range_monthly(DATASET_CONS, CONS_START, CONS_END)\n",
    "print(\"Consumption data shape:\", df_cons.shape)\n",
    "print(df_cons.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f282abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production DataFrame ready: (657600, 4)\n",
      "Consumption DataFrame ready: (876600, 4)\n",
      "Production group names: ['hydro' 'wind' 'other' 'thermal' 'solar']\n",
      "Consumption group names: ['cabin' 'tertiary' 'household' 'secondary' 'primary']\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# FIX DATAFRAMES FOR SPARK / CASSANDRA\n",
    "# ============================\n",
    "\n",
    "# 1Ô∏è‚É£ Reset index so 'startTime' becomes a column\n",
    "df_prod = df_prod.reset_index()   # 'startTime' moves from index ‚Üí column\n",
    "df_cons = df_cons.reset_index()\n",
    "\n",
    "# 2Ô∏è‚É£ Strip any extra whitespace from column names\n",
    "df_prod.columns = df_prod.columns.str.strip()\n",
    "df_cons.columns = df_cons.columns.str.strip()\n",
    "\n",
    "# 3Ô∏è‚É£ Ensure uniform group column name\n",
    "# (In your data, it's already 'groupName', but this covers any variations)\n",
    "if 'productionGroup' in df_prod.columns:\n",
    "    df_prod = df_prod.rename(columns={\"productionGroup\": \"groupName\"})\n",
    "if 'consumptionGroup' in df_cons.columns:\n",
    "    df_cons = df_cons.rename(columns={\"consumptionGroup\": \"groupName\"})\n",
    "\n",
    "# 4Ô∏è‚É£ Ensure proper types\n",
    "df_prod['startTime'] = pd.to_datetime(df_prod['startTime'], utc=True, errors='coerce')\n",
    "df_prod['quantityKwh'] = pd.to_numeric(df_prod['quantityKwh'], errors='coerce')\n",
    "\n",
    "df_cons['startTime'] = pd.to_datetime(df_cons['startTime'], utc=True, errors='coerce')\n",
    "df_cons['quantityKwh'] = pd.to_numeric(df_cons['quantityKwh'], errors='coerce')\n",
    "\n",
    "# 5Ô∏è‚É£ Drop rows with missing critical values\n",
    "df_prod = df_prod.dropna(subset=['startTime','priceArea','groupName','quantityKwh'])\n",
    "df_cons = df_cons.dropna(subset=['startTime','priceArea','groupName','quantityKwh'])\n",
    "\n",
    "# 6Ô∏è‚É£ Optional: drop 'endTime' if not needed in Cassandra\n",
    "df_prod = df_prod.drop(columns=['endTime'], errors='ignore')\n",
    "df_cons = df_cons.drop(columns=['endTime'], errors='ignore')\n",
    "\n",
    "# ‚úÖ Check that DataFrames are ready\n",
    "print(\"Production DataFrame ready:\", df_prod.shape)\n",
    "print(\"Consumption DataFrame ready:\", df_cons.shape)\n",
    "print(\"Production group names:\", df_prod['groupName'].unique())\n",
    "print(\"Consumption group names:\", df_cons['groupName'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a515369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Keyspace and tables for production and consumption are ready\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "# Connect to local Cassandra\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "session = cluster.connect()\n",
    "\n",
    "# --- Create keyspace ---\n",
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS energy\n",
    "    WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\n",
    "\"\"\")\n",
    "\n",
    "# --- Create table for production ---\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS energy.production_per_group (\n",
    "        priceArea text,\n",
    "        startTime timestamp,\n",
    "        productionGroup text,\n",
    "        quantityKwh double,\n",
    "        PRIMARY KEY ((priceArea), startTime, productionGroup)\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# --- Create table for consumption ---\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS energy.consumption_per_group (\n",
    "        priceArea text,\n",
    "        startTime timestamp,\n",
    "        consumptionGroup text,\n",
    "        quantityKwh double,\n",
    "        PRIMARY KEY ((priceArea), startTime, consumptionGroup)\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Keyspace and tables for production and consumption are ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebbf7976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ startTime timezone fixed to UTC for both DataFrames\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Ensure startTime is timezone-aware UTC\n",
    "# -------------------------------\n",
    "\n",
    "import pytz\n",
    "\n",
    "# Production\n",
    "if df_prod['startTime'].dt.tz is None:\n",
    "    df_prod['startTime'] = df_prod['startTime'].dt.tz_localize('UTC')\n",
    "else:\n",
    "    df_prod['startTime'] = df_prod['startTime'].dt.tz_convert('UTC')\n",
    "\n",
    "# Consumption\n",
    "if df_cons['startTime'].dt.tz is None:\n",
    "    df_cons['startTime'] = df_cons['startTime'].dt.tz_localize('UTC')\n",
    "else:\n",
    "    df_cons['startTime'] = df_cons['startTime'].dt.tz_convert('UTC')\n",
    "\n",
    "print(\"‚úÖ startTime timezone fixed to UTC for both DataFrames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ed07029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n",
      "Writing 657600 rows in 7 chunks for production...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 13:40:46 WARN TaskSetManager: Stage 32 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 1/7 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 13:40:48 WARN TaskSetManager: Stage 33 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 2/7 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 13:40:49 WARN TaskSetManager: Stage 34 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 3/7 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 13:40:51 WARN TaskSetManager: Stage 35 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 4/7 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 13:40:52 WARN TaskSetManager: Stage 36 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 5/7 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 13:40:54 WARN TaskSetManager: Stage 37 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 6/7 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 13:40:55 WARN TaskSetManager: Stage 38 contains a task of very large size (1993 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 7/7 written (57600 rows)\n",
      "Writing 876600 rows in 9 chunks for consumption...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 13:40:56 WARN TaskSetManager: Stage 39 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 1/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 13:40:58 WARN TaskSetManager: Stage 40 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 2/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 13:40:59 WARN TaskSetManager: Stage 41 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 3/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 13:41:01 WARN TaskSetManager: Stage 42 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 4/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 13:41:03 WARN TaskSetManager: Stage 43 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 5/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 13:41:04 WARN TaskSetManager: Stage 44 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 6/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 13:41:06 WARN TaskSetManager: Stage 45 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 7/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 13:41:07 WARN TaskSetManager: Stage 46 contains a task of very large size (3686 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 8/9 written (100000 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 13:41:09 WARN TaskSetManager: Stage 47 contains a task of very large size (2826 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 47:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk 9/9 written (76600 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType\n",
    "import math\n",
    "\n",
    "# --- CREATE SPARK SESSION ---\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ElhubDataIngest\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "# --- DEFINE COMMON SCHEMA ---\n",
    "schema = StructType([\n",
    "    StructField(\"priceArea\", StringType(), True),\n",
    "    StructField(\"groupName\", StringType(), True),\n",
    "    StructField(\"startTime\", TimestampType(), True),\n",
    "    StructField(\"quantityKwh\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# --- OPTIMIZED FUNCTION TO WRITE IN CHUNKS ---\n",
    "def write_to_cassandra_fast(df_pandas, dataset_type=\"production\", chunk_size=100_000):\n",
    "    if df_pandas is None or df_pandas.empty:\n",
    "        print(f\"No data to write for {dataset_type}\")\n",
    "        return\n",
    "\n",
    "    # Drop any leftover index columns\n",
    "    df_pandas = df_pandas.drop(columns=['index', 'level_0'], errors='ignore')\n",
    "\n",
    "    total_rows = len(df_pandas)\n",
    "    n_chunks = math.ceil(total_rows / chunk_size)\n",
    "    print(f\"Writing {total_rows} rows in {n_chunks} chunks for {dataset_type}...\")\n",
    "\n",
    "    for i in range(n_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i+1) * chunk_size, total_rows)\n",
    "        df_chunk = df_pandas.iloc[start_idx:end_idx]\n",
    "\n",
    "        # Make sure the columns match schema order exactly\n",
    "        df_chunk_spark = df_chunk[['priceArea', 'groupName', 'startTime', 'quantityKwh']]\n",
    "\n",
    "        # Convert to Spark DataFrame\n",
    "        spark_df = spark.createDataFrame(df_chunk_spark, schema=schema)\n",
    "\n",
    "        # Rename columns for Cassandra\n",
    "        if dataset_type == \"production\":\n",
    "            spark_df_cassandra = spark_df.selectExpr(\n",
    "                \"priceArea as pricearea\",\n",
    "                \"startTime as starttime\",\n",
    "                \"groupName as productiongroup\",\n",
    "                \"quantityKwh as quantitykwh\"\n",
    "            )\n",
    "            table = \"production_per_group\"\n",
    "        else:\n",
    "            spark_df_cassandra = spark_df.selectExpr(\n",
    "                \"priceArea as pricearea\",\n",
    "                \"startTime as starttime\",\n",
    "                \"groupName as consumptiongroup\",\n",
    "                \"quantityKwh as quantitykwh\"\n",
    "            )\n",
    "            table = \"consumption_per_group\"\n",
    "\n",
    "        # Reduce Spark partitions to 1 per chunk for faster small writes\n",
    "        spark_df_cassandra.coalesce(1).write \\\n",
    "            .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "            .options(keyspace=\"energy\", table=table) \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "\n",
    "        print(f\"‚úÖ Chunk {i+1}/{n_chunks} written ({len(df_chunk)} rows)\")\n",
    "\n",
    "# --- WRITE BOTH DATASETS ---\n",
    "write_to_cassandra_fast(df_prod, \"production\")\n",
    "write_to_cassandra_fast(df_cons, \"consumption\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4321e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Production table preview:\n",
      "+---------+-------------------+---------------+-----------+\n",
      "|pricearea|starttime          |productiongroup|quantitykwh|\n",
      "+---------+-------------------+---------------+-----------+\n",
      "|NO1      |2021-01-01 00:00:00|hydro          |2507716.8  |\n",
      "|NO1      |2021-01-01 00:00:00|other          |0.0        |\n",
      "|NO1      |2021-01-01 00:00:00|solar          |6.106      |\n",
      "|NO1      |2021-01-01 00:00:00|thermal        |51369.035  |\n",
      "|NO1      |2021-01-01 00:00:00|wind           |937.072    |\n",
      "+---------+-------------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows in production_per_group: 872853\n",
      "\n",
      "‚úÖ Consumption table preview:\n",
      "+---------+-------------------+----------------+-----------+\n",
      "|pricearea|starttime          |consumptiongroup|quantitykwh|\n",
      "+---------+-------------------+----------------+-----------+\n",
      "|NO2      |2021-01-01 00:00:00|cabin           |142641.7   |\n",
      "|NO2      |2021-01-01 00:00:00|household       |1425141.0  |\n",
      "|NO2      |2021-01-01 00:00:00|primary         |66907.58   |\n",
      "|NO2      |2021-01-01 00:00:00|secondary       |2121090.2  |\n",
      "|NO2      |2021-01-01 00:00:00|tertiary        |608354.44  |\n",
      "+---------+-------------------+----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows in consumption_per_group: 876500\n",
      "\n",
      "Production groups in Cassandra: [Row(productiongroup='solar'), Row(productiongroup='other'), Row(productiongroup='thermal'), Row(productiongroup='hydro'), Row(productiongroup='wind')]\n",
      "Consumption groups in Cassandra: [Row(consumptiongroup='tertiary'), Row(consumptiongroup='secondary'), Row(consumptiongroup='household'), Row(consumptiongroup='cabin'), Row(consumptiongroup='primary')]\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3Ô∏è‚É£ READ DATA BACK FROM CASSANDRA\n",
    "# ==========================================\n",
    "def read_from_cassandra(table_name):\n",
    "    \"\"\"Reads a table from the 'energy' keyspace into a Spark DataFrame.\"\"\"\n",
    "    df = spark.read \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(keyspace=\"energy\", table=table_name) \\\n",
    "        .load()\n",
    "    return df\n",
    "\n",
    "# --- Read production table ---\n",
    "cass_prod_df = read_from_cassandra(\"production_per_group\")\n",
    "print(\"‚úÖ Production table preview:\")\n",
    "cass_prod_df.show(5, truncate=False)\n",
    "\n",
    "prod_count = cass_prod_df.count()\n",
    "print(f\"Total rows in production_per_group: {prod_count}\")\n",
    "\n",
    "# --- Read consumption table ---\n",
    "cass_cons_df = read_from_cassandra(\"consumption_per_group\")\n",
    "print(\"\\n‚úÖ Consumption table preview:\")\n",
    "cass_cons_df.show(5, truncate=False)\n",
    "\n",
    "cons_count = cass_cons_df.count()\n",
    "print(f\"Total rows in consumption_per_group: {cons_count}\")\n",
    "\n",
    "# --- Optional: basic sanity check on groups ---\n",
    "print(\"\\nProduction groups in Cassandra:\", cass_prod_df.select(\"productiongroup\").distinct().collect())\n",
    "print(\"Consumption groups in Cassandra:\", cass_cons_df.select(\"consumptiongroup\").distinct().collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18ee26b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient, UpdateOne\n",
    "import math\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# === MongoDB Atlas connection ===\n",
    "uri = \"mongodb+srv://{}:{}@cluster0.qwrlccf.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "USR, PWD = open('/Users/sarahorte/Documents/IND320/Personlig/No_sync/MongoDB').read().splitlines()\n",
    "client = MongoClient(uri.format(USR, PWD))\n",
    "database = client['elhub']\n",
    "\n",
    "def upsert_chunk(df_chunk, collection_name):\n",
    "    collection = database[collection_name]\n",
    "    records = df_chunk.to_dict(orient='records')\n",
    "    operations = [\n",
    "        UpdateOne(\n",
    "            {\n",
    "                \"pricearea\": rec[\"priceArea\"],\n",
    "                \"starttime\": rec[\"startTime\"],\n",
    "                \"productiongroup\": rec.get(\"groupName\")\n",
    "            },\n",
    "            {\"$set\": rec},\n",
    "            upsert=True\n",
    "        )\n",
    "        for rec in records\n",
    "    ]\n",
    "    if operations:\n",
    "        collection.bulk_write(operations)\n",
    "    return len(df_chunk)  # for progress tracking\n",
    "\n",
    "def upsert_into_mongo_parallel(df, collection_name, chunk_size=50_000, max_workers=4):\n",
    "    total_rows = len(df)\n",
    "    n_chunks = math.ceil(total_rows / chunk_size)\n",
    "    print(f\"Upserting {total_rows} documents to '{collection_name}' in {n_chunks} chunks using {max_workers} workers...\")\n",
    "\n",
    "    chunks = [df.iloc[i*chunk_size : min((i+1)*chunk_size, total_rows)] for i in range(n_chunks)]\n",
    "    inserted_so_far = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(upsert_chunk, chunk, collection_name): i for i, chunk in enumerate(chunks)}\n",
    "        for future in as_completed(futures):\n",
    "            inserted = future.result()\n",
    "            inserted_so_far += inserted\n",
    "            percent = inserted_so_far / total_rows * 100\n",
    "            print(f\"‚úÖ {inserted_so_far}/{total_rows} documents upserted ({percent:.1f}% done)\")\n",
    "\n",
    "    total_docs = database[collection_name].count_documents({})\n",
    "    print(f\"‚úÖ Finished. Total documents in '{collection_name}': {total_docs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0c76829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 657600 documents to 'production_data' in 7 chunks...\n",
      "‚úÖ Chunk 1/7 inserted (100000 docs)\n",
      "‚úÖ Chunk 2/7 inserted (100000 docs)\n",
      "‚úÖ Chunk 3/7 inserted (100000 docs)\n",
      "‚úÖ Chunk 4/7 inserted (100000 docs)\n",
      "‚úÖ Chunk 5/7 inserted (100000 docs)\n",
      "‚úÖ Chunk 6/7 inserted (100000 docs)\n",
      "‚úÖ Chunk 7/7 inserted (57600 docs)\n",
      "‚úÖ Total documents in 'production_data': 872953\n",
      "üóë Cleared existing data in 'consumption_data'\n",
      "Writing 876600 documents to 'consumption_data' in 9 chunks...\n",
      "‚úÖ Chunk 1/9 inserted (100000 docs)\n",
      "‚úÖ Chunk 2/9 inserted (100000 docs)\n",
      "‚úÖ Chunk 3/9 inserted (100000 docs)\n",
      "‚úÖ Chunk 4/9 inserted (100000 docs)\n",
      "‚úÖ Chunk 5/9 inserted (100000 docs)\n",
      "‚úÖ Chunk 6/9 inserted (100000 docs)\n",
      "‚úÖ Chunk 7/9 inserted (100000 docs)\n",
      "‚úÖ Chunk 8/9 inserted (100000 docs)\n",
      "‚úÖ Chunk 9/9 inserted (76600 docs)\n",
      "‚úÖ Total documents in 'consumption_data': 876600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 14:40:57 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|id: 0x538c207b, L:/127.0.0.1:62967 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:40:57 WARN ControlConnection: [s0] Error connecting to Node(endPoint=localhost/127.0.0.1:9042, hostId=3eb45f82-fad5-441b-80c8-53ce36700638, hashCode=201e6c65), trying next node (ConnectionInitException: [s0|control|id: 0x626d8841, L:/127.0.0.1:62960 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:40:57 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|id: 0x5613bb9b, L:/127.0.0.1:62968 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:40:57 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|id: 0xb7e63310, L:/127.0.0.1:62961 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:40:57 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|id: 0xc2798f53, L:/127.0.0.1:62969 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:40:57 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|id: 0xd0165f1f, L:/127.0.0.1:62963 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:40:57 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|id: 0xcabd787b, L:/127.0.0.1:62962 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:40:57 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|id: 0x724a9c83, L:/127.0.0.1:62965 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:40:57 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|id: 0x681fbf1c, L:/127.0.0.1:62964 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:40:57 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|id: 0x654243d0, L:/127.0.0.1:62966 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:40:58 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|id: 0x2b464e8d, L:/127.0.0.1:62976 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:40:58 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|id: 0xab993ca7, L:/127.0.0.1:62977 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:40:58 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|id: 0xbedadb68, L:/127.0.0.1:62978 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:40:58 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|id: 0x98e0ca1a, L:/127.0.0.1:62979 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:40:58 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|id: 0x2721ffaf, L:/127.0.0.1:62980 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:40:58 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|id: 0x9850a297, L:/127.0.0.1:62981 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:40:58 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|id: 0x21e75ca3, L:/127.0.0.1:62983 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:40:58 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|id: 0x2dac952b, L:/127.0.0.1:62982 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:40:58 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|id: 0x31c16ae4, L:/127.0.0.1:62984 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:40:59 WARN ControlConnection: [s0] Error connecting to Node(endPoint=localhost/127.0.0.1:9042, hostId=3eb45f82-fad5-441b-80c8-53ce36700638, hashCode=201e6c65), trying next node (ConnectionInitException: [s0|control|id: 0x30c2d82e, L:/127.0.0.1:62986 - R:localhost/127.0.0.1:9042] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): unexpected failure (com.datastax.oss.driver.api.core.connection.ClosedConnectionException: Unexpected error on channel))\n",
      "25/11/11 14:41:02 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/11/11 14:41:02 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/11/11 14:41:02 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/11/11 14:41:02 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/11/11 14:41:02 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/11/11 14:41:02 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/11/11 14:41:02 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/11/11 14:41:02 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/11/11 14:41:02 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/11/11 14:41:03 WARN ControlConnection: [s0] Error connecting to Node(endPoint=localhost/127.0.0.1:9042, hostId=3eb45f82-fad5-441b-80c8-53ce36700638, hashCode=201e6c65), trying next node (ConnectionInitException: [s0|control|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/11/11 14:41:11 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/11/11 14:41:11 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/11/11 14:41:11 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/11/11 14:41:11 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/11/11 14:41:11 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/11/11 14:41:11 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/11/11 14:41:11 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/11/11 14:41:11 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/11/11 14:41:11 WARN ChannelPool: [s0|/127.0.0.1:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=39d191bd-1616-4bc4-9bb4-3f5b832a6fd6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1762863743111}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import math\n",
    "\n",
    "# === MongoDB Atlas connection ===\n",
    "uri = \"mongodb+srv://{}:{}@cluster0.qwrlccf.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "USR, PWD = open('/Users/sarahorte/Documents/IND320/Personlig/No_sync/MongoDB').read().splitlines()\n",
    "client = MongoClient(uri.format(USR, PWD))\n",
    "database = client['elhub']\n",
    "\n",
    "# --- Fast bulk insert function ---\n",
    "def bulk_insert(df, collection_name, chunk_size=100_000, clear_existing=False):\n",
    "    collection = database[collection_name]\n",
    "    if clear_existing:\n",
    "        collection.delete_many({})\n",
    "        print(f\"üóë Cleared existing data in '{collection_name}'\")\n",
    "\n",
    "    total_rows = len(df)\n",
    "    n_chunks = math.ceil(total_rows / chunk_size)\n",
    "    print(f\"Writing {total_rows} documents to '{collection_name}' in {n_chunks} chunks...\")\n",
    "\n",
    "    for i in range(n_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i+1) * chunk_size, total_rows)\n",
    "        df_chunk = df.iloc[start_idx:end_idx]\n",
    "        records = df_chunk.to_dict(orient='records')\n",
    "        collection.insert_many(records)\n",
    "        print(f\"‚úÖ Chunk {i+1}/{n_chunks} inserted ({len(df_chunk)} docs)\")\n",
    "\n",
    "    total_docs = collection.count_documents({})\n",
    "    print(f\"‚úÖ Total documents in '{collection_name}': {total_docs}\")\n",
    "\n",
    "# --- Convert Spark DataFrames to Pandas ---\n",
    "df_prod_mongo = df_prod.copy()  # 2022‚Äì2024 production\n",
    "df_cons_mongo = df_cons.copy()  # 2021‚Äì2024 consumption\n",
    "\n",
    "# --- Run fast insertion ---\n",
    "bulk_insert(df_prod_mongo, 'production_data', clear_existing=False)  # keep 2021, add 2022‚Äì2024\n",
    "bulk_insert(df_cons_mongo, 'consumption_data', clear_existing=True)  # clear & reload\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2D_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
